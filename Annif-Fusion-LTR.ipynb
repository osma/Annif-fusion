{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annif Fusion experiment\n",
    "\n",
    "**Osma Suominen, October 2018**\n",
    "\n",
    "This notebook is an experiment for evaluating different methods to combine results of multiple subject indexing algorithms. We will use a document collection where gold standard subjects have been manually assigned and compare the subjects assigned by three different algorithm first individually, then in combinations created by different fusion approaches. In particular, we will test simple mean-of-subject-scores and union-of-top-K approaches as well as more advanced methods such as score normalization, isotonic regression and Learning to Rank style machine learning. Ideally, we would like to find a method for combining results from multiple algorithm that gives us the best quality results, combining the strengths of individual algorithms (both statistical/associative and lexical) while eliminating the effects of their weaknesses. The findings will then inform the further development of Annif.\n",
    "\n",
    "The experiment was inspired by the Martin Toepfer's paper [Fusion architectures for automatic subject indexing under concept drift](https://link.springer.com/article/10.1007/s00799-018-0240-3) as well as discussions with him during and after the NKOS2018 workshop.\n",
    "\n",
    "If you want to run this yourself, you need Python 3.5+ with the following libraries:\n",
    "\n",
    "* jupyter (obviously)\n",
    "* scikit-learn\n",
    "* pyltr\n",
    "* matplotlib (for the final plots)\n",
    "\n",
    "This document is (c) Osma Suominen. It may be shared and reused according to the terms of the [CC Attribution 4.0 International](https://creativecommons.org/licenses/by/4.0/) license. Attribution should include the name of the author and a link to the original source document. However, code snippets in this notebook may be freely reused according to the [CC0 1.0 Universal](https://creativecommons.org/publicdomain/zero/1.0/) license. Attribution for code is requested but not required.\n",
    "\n",
    "## Document corpus\n",
    "\n",
    "We will use the [\"Ask a Librarian\" document corpus](https://github.com/NatLibFi/Annif-corpora/tree/master/fulltext/kirjastonhoitaja) for the experiment. It contains 3150 short question-answer pairs in Finnish language. The collection is subdivided into three subsets: train (n=2625), validate (n=213) and test (n=312). All the final evaluations will be performed on the test set, which contains the most recent questions asked during the year 2017. However, for some of the fusion methods we will make use of the train and validate subsets in order to fine-tune the way results are combined.\n",
    "\n",
    "All documents in the collection have been assigned gold standard subjects by librarians, stored in `*.tsv` files where the basenames correspond to the original `*.txt` files (which are not read by this code at all and are not included in this repository). Librarians have assigned at least 4 subjects per document; the average is 4.4-4.8 subjects per document, depending on the subset.\n",
    "\n",
    "For the jyu-theses-fin data set, there are around 5.9 gold standard subjects per document.\n",
    "\n",
    "In addition, the documents have been analyzed using the [Annif](https://github.com/NatLibFi/Annif/) tool (development version v0.31.0) using three independent automated subject indexing algorithms: TF-IDF vector similarity, fastText and Maui, asking each algorithm to suggest up to 1000 subjects per document (however, we will in practice only load a fraction of these) with scores ranging from 1.0 to 0.0 given to each suggested subject. The subjects suggested by these algoritms are stored, respectively, in `*.tfidf`, `*.fasttext` and `*.maui` files.\n",
    "\n",
    "First we load some basic modules and define the locations of the data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyltr\n",
    "import os\n",
    "import os.path\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "# Select the dataset to use for the analysis by uncommenting one of these lines\n",
    "#ds = 'kirjastonhoitaja' # Ask a Librarian dataset\n",
    "ds = 'jyu-theses-fin'   # JYU Theses dataset\n",
    "\n",
    "SUBJECTFILE='data/yso.tsv'\n",
    "TRAIN_DIR='data/{}/train/'.format(ds)\n",
    "VALI_DIR='data/{}/validate/'.format(ds)\n",
    "TEST_DIR='data/{}/test/'.format(ds)\n",
    "FILE_SIZES='data/{}/file-sizes.txt'.format(ds)\n",
    "\n",
    "# define the number of concepts for which to calculate F1 score; this varies by dataset\n",
    "F1_LIMIT = {'kirjastonhoitaja': 5, 'jyu-theses-fin': 10}\n",
    "\n",
    "ALGORITHMS = ('tfidf','fasttext','maui')  # these correspond to file extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject vocabulary\n",
    "\n",
    "All the subjects have been chosen from the Finnish General Ontology YSO, which contains around 28000 concepts.\n",
    "Here we load the vocabulary from a TSV file where the first column is the concept URI, the second column is the number of training documents collected from the Finna.fi corpus and used to train the tf-idf and fastText algorithms, and the third column is the concept label in Finnish. This has been extracted from the full YSO SKOS file and combined with frequencies from the YSO-Finna corpus.\n",
    "\n",
    "Hereafter we will only use integer concept IDs which range from 0 to (n_concepts-1). All calculations are performed using the concept IDs so we just need to map the concept URIs which appear in files to their IDs.\n",
    "\n",
    "In addition, we store the concept labels for later use as features for the Learning to Rank algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary loaded with 27760 concepts\n"
     ]
    }
   ],
   "source": [
    "uri_to_cid = {}\n",
    "subject_label = {}\n",
    "subject_freq_finna = []\n",
    "with open(SUBJECTFILE) as subjf:\n",
    "    for cid, line in enumerate(subjf):\n",
    "        uri, freq, label = line.strip().split(\"\\t\")\n",
    "        uri_to_cid[uri] = cid\n",
    "        subject_freq_finna.append(int(freq))\n",
    "        subject_label[cid] = label\n",
    "n_concepts = len(uri_to_cid)\n",
    "print(\"Vocabulary loaded with\", n_concepts, \"concepts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document corpus\n",
    "\n",
    "We define a function to load the data from a directory of files in the format explained above. To be able to use both `scikit-learn` and `pyltr` libraries, we will need to express the data in two somewhat different formats, both based on multidimensional NumPy arrays:\n",
    "    \n",
    "1. pyltr ranking format, where the NumPy array rows correspond to document-subject pairs, i.e. there are multiple rows per document, one row per subject that has been suggested by at least one algorithm. A separate qids array (\"query IDs\", actually document IDs when applied this way) maps the document-subject pairs to the individual documents.\n",
    "2. scikit-learn multilabel format, where the NumPy array has one row per document and the columns are individual concepts (either True/False for the gold standard subjects, or subject scores for the predicted subjects)\n",
    "\n",
    "We will define this function once and then use it to parse all three document subsets (train, validate, test). We will only load up to 25 suggested subjects per document to keep the size of the rankings manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TX shape: (428445, 3)\n",
      "Ty shape: (428445,)\n",
      "Tqids shape: (428445,)\n",
      "Tcids shape: (428445,)\n",
      "Ttrue shape: (3635, 27760)\n",
      "Tscores shape: (3635, 27760, 3)\n",
      "\n",
      "VX shape: (92779, 3)\n",
      "Vy shape: (92779,)\n",
      "Vqids shape: (92779,)\n",
      "Vcids shape: (92779,)\n",
      "Vtrue shape: (786, 27760)\n",
      "Vscores shape: (786, 27760, 3)\n",
      "\n",
      "EX shape: (91044, 3)\n",
      "Ey shape: (91044,)\n",
      "Eqids shape: (91044,)\n",
      "Ecids shape: (91044,)\n",
      "Etrue shape: (766, 27760)\n",
      "Escores shape: (766, 27760, 3)\n"
     ]
    }
   ],
   "source": [
    "def load_data(directory, max_pred=50):\n",
    "    \"\"\"Reads document corpus with gold standard and predicted subjects for each document.\n",
    "    Only the top max_pred predictions per document and algorithm are considered for ranking.\n",
    "    Returns a tuple with\n",
    "     - predictions of individual algorithms as a 2D NumPy array with the shape (doc-suggestions, n_algos)\n",
    "     - relevant (gold standard) concepts as a 1D NumPy array with the shape (doc-suggestions)\n",
    "     - qids (actually document ids) as a 1D NumPy array with length doc-suggestions\n",
    "     - concept IDs of suggested concepts as a 1D NumPy array with shape (doc-suggestions)\n",
    "     - gold standard subjects in sklearn multilabel format as a 2D NumPy array with shape (n_docs, n_concepts) \n",
    "     - predictions of individual algorithms in sklearn multilabel format as a 3D NumPy array with shape (n_docs, n_concepts, n_algos)\n",
    "    \"\"\"\n",
    "    \n",
    "    # temporary lists to be converted into NumPy arrays at the end\n",
    "    docdatalist = []\n",
    "    docscorelist = []\n",
    "    docgoldlist = []\n",
    "    doctruelist = []\n",
    "    qidlist = []\n",
    "    cidlist = []\n",
    "    \n",
    "    for tsvfilename in glob.glob(os.path.join(directory, '*.tsv')):\n",
    "        basename = tsvfilename[:-4]\n",
    "        \n",
    "        # Initialize binary mask containing the top max_pred concepts that were suggested by any algorithm.\n",
    "        # This will be used to pare down the doc-suggestions for ranking so that we will skip concepts\n",
    "        # that were not suggested by any algorithm. They will still be considered in F1 score evaluation,\n",
    "        # which is not based only on the ranked suggestions but used the full gold standard subjects.\n",
    "        mask = np.zeros(n_concepts, dtype=np.bool)\n",
    "\n",
    "        docdata = np.zeros((n_concepts, len(ALGORITHMS)))\n",
    "        for algoid, fileext in enumerate(ALGORITHMS):\n",
    "            with open(basename + \".\" + fileext) as algooutput:\n",
    "                lines = 0\n",
    "                for lineno, line in enumerate(algooutput):\n",
    "                    uri, _, score = line.strip().split(\"\\t\")\n",
    "                    if uri not in uri_to_cid:\n",
    "                        continue  # ignore unknown URIs\n",
    "                    cid = uri_to_cid[uri]\n",
    "                    docdata[cid,algoid] = float(score)\n",
    "                    if lineno < max_pred:\n",
    "                        mask[cid] = True\n",
    "        docdatalist.append(docdata[mask])\n",
    "        docscorelist.append(docdata)\n",
    "    \n",
    "        docgold = np.zeros(n_concepts, dtype=np.bool)\n",
    "        with open(tsvfilename) as tsvfile:\n",
    "            for line in tsvfile:\n",
    "                uri = line.split(\"\\t\")[0]\n",
    "                if uri in uri_to_cid:\n",
    "                    cid = uri_to_cid[uri]\n",
    "                    docgold[cid] = True\n",
    "        docgoldlist.append(docgold[mask])\n",
    "        doctruelist.append(docgold)\n",
    "        \n",
    "        fileid = int(basename.split('/')[-1].split('-')[-1])\n",
    "        qidlist.append(np.full(mask.sum(), fileid))\n",
    "        cidlist.append(np.arange(n_concepts)[mask])\n",
    "\n",
    "    return np.concatenate(docdatalist), np.concatenate(docgoldlist), np.concatenate(qidlist), \\\n",
    "           np.concatenate(cidlist), np.array(doctruelist), np.array(docscorelist)\n",
    "\n",
    "# now load the train, validate and test documents\n",
    "TX, Ty, Tqids, Tcids, Ttrue, Tscores = load_data(TRAIN_DIR)\n",
    "VX, Vy, Vqids, Vcids, Vtrue, Vscores = load_data(VALI_DIR)\n",
    "EX, Ey, Eqids, Ecids, Etrue, Escores = load_data(TEST_DIR)\n",
    "\n",
    "# Show some information about the shapes\n",
    "print(\"TX shape:\", TX.shape)\n",
    "print(\"Ty shape:\", Ty.shape)\n",
    "print(\"Tqids shape:\", Tqids.shape)\n",
    "print(\"Tcids shape:\", Tcids.shape)\n",
    "print(\"Ttrue shape:\", Ttrue.shape)\n",
    "print(\"Tscores shape:\", Tscores.shape)\n",
    "print()\n",
    "print(\"VX shape:\", VX.shape)\n",
    "print(\"Vy shape:\", Vy.shape)\n",
    "print(\"Vqids shape:\", Vqids.shape)\n",
    "print(\"Vcids shape:\", Vcids.shape)\n",
    "print(\"Vtrue shape:\", Vtrue.shape)\n",
    "print(\"Vscores shape:\", Vscores.shape)\n",
    "print()\n",
    "print(\"EX shape:\", EX.shape)\n",
    "print(\"Ey shape:\", Ey.shape)\n",
    "print(\"Eqids shape:\", Eqids.shape)\n",
    "print(\"Ecids shape:\", Ecids.shape)\n",
    "print(\"Etrue shape:\", Etrue.shape)\n",
    "print(\"Escores shape:\", Escores.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics\n",
    "\n",
    "For evaluating the quality of results, we will use two metrics: [NDCG](https://en.wikipedia.org/wiki/Discounted_cumulative_gain#Normalized_DCG) which is based on ranked suggestions (if the top ranked suggestion is correct it will contribute more to the score than getting the 2nd or 3rd suggestion right) and [F1 score](https://en.wikipedia.org/wiki/F1_score) that only considers binary choices: either a concept is a subject of a document or it is not, there are no fuzzy options. \n",
    "\n",
    "For the **NDCG score**, we can simply use the implementation from the `pyltr` library. Here we limit the NDCG evaluation to the top 20 most highly ranked subjects (i.e. NDCG@20), meaning that subjects ranked 21st, 22nd etc won't contribute to the score (their effect would be quite small anyway do to the discounting performed in NDCG calculations).\n",
    "\n",
    "For the **F1 score**, we can use the `scikit-learn` implementation. However, its usage is complicated by the fact that we are mostly dealing with ranked suggestions in the pyltr format, so we need to convert those into the multilabel prediction format understood by scikit-learn and further binarize the predictions using a thresholding strategy. For simplicity we will pick the top K suggested concepts (where K varies by dataset), so we get an amount of subjects that is similar to the librarian-assigned ones (around 4.5 subjects per document for the kirjastonhoitaja data set), and compare those to the gold standard. The score for each document will be calculated individually and the final result will be the average of those scores (i.e. sample based average).\n",
    "\n",
    "Finally, we will define an evaluation function that is given ranked suggestions as well as gold standard subjects (in both pyltr and scikit-learn formats) and some auxiliary information (qids and cids). It will calculate both NDCG and F1 score, print them out, and also return the scores so that they can be stored for later plotting etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# initialize the NDCG metric implementation\n",
    "ndcg_metric = pyltr.metrics.NDCG(k=20)\n",
    "\n",
    "def ranking_to_multilabel(pred, qids, cids):\n",
    "    \"\"\"convert from pyltr ranking prediction format to sklearn multilabel prediction format\"\"\"\n",
    "    doc_labels = collections.OrderedDict()  # key: qid, val: NumPy 1D array with length n_concepts\n",
    "    for score, qid, cid in zip(pred, qids, cids):\n",
    "        if qid not in doc_labels:\n",
    "            doc_labels[qid] = np.zeros(n_concepts)\n",
    "        doc_labels[qid][cid] = score\n",
    "    return np.array(list(doc_labels.values()))\n",
    "\n",
    "def binarize_multilabel_pred(pred, k=5):\n",
    "    \"\"\"convert predictions with scores to a boolean matrix, taking the top K predictions per document\"\"\"\n",
    "    # I'm pretty sure this could be done easier, perhaps in a single NumPy operation, but I can't figure it out\n",
    "    # Using a loop instead, it's not going to take long anyway\n",
    "    binary_labels = np.zeros(pred.shape, dtype=np.bool)\n",
    "    pred_order = pred.argsort()[:,::-1]\n",
    "    for i, mask in enumerate(pred_order[:,:k]):\n",
    "        binary_labels[i,mask] = True\n",
    "    return binary_labels & (pred > 0.0)  # make sure not to include predictions with 0.0 score\n",
    "\n",
    "def evaluate(name, qids, y, y_true, pred, cids):\n",
    "    \"\"\"evaluate a ranking, calculating its NDCG and F1 score. Print and return the scores\"\"\"\n",
    "    # calculate NDCG score\n",
    "    ndcg = ndcg_metric.calc_mean(qids, y, pred)\n",
    "\n",
    "    # calculate F1 score\n",
    "    pred_labels = ranking_to_multilabel(pred, qids, cids)\n",
    "    f1 = f1_score(y_true, binarize_multilabel_pred(pred_labels, k=F1_LIMIT[ds]), average='samples')\n",
    "    \n",
    "    print(\"{:>20}:\\tNDCG={:.4f}\\tF1@{}={:.4f}\".format(name, ndcg, F1_LIMIT[ds], f1))\n",
    "    return ndcg, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline evaluation\n",
    "\n",
    "Now we are ready for some baseline evaluations: we can evaluate the individual algorithms as well as a simplistic combination method using the mean of assigned scores (the only one implemented in Annif v0.31.0). We will store the results for later too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               tfidf:\tNDCG=0.4813\tF1@10=0.2595\n",
      "            fasttext:\tNDCG=0.4798\tF1@10=0.2681\n",
      "                maui:\tNDCG=0.6592\tF1@10=0.3864\n",
      "                mean:\tNDCG=0.6952\tF1@10=0.4047\n"
     ]
    }
   ],
   "source": [
    "# initialize dictionary for collecting evaluation results\n",
    "eval_results = collections.OrderedDict()\n",
    "\n",
    "# evaluate the individual algorithms\n",
    "for algoid, name in enumerate(ALGORITHMS):\n",
    "    eval_results[name] = evaluate(name, Eqids, Ey, Etrue, EX[:,algoid], Ecids)\n",
    "\n",
    "# evaluate the combination of algorithms using mean of scores\n",
    "eval_results[\"mean\"] = evaluate(\"mean\", Eqids, Ey, Etrue, EX.mean(1), Ecids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that out of individual algorithms, Maui performed best, followed by TF-IDF and fastText. The mean of scores gave better results than any algorithm alone. The NDCG and F1 measures seem to agree on which methods are better than others, as they both ranked the methods in the same order.\n",
    "\n",
    "Note that the F1 scores may seem quite low (in some classification tasks F1 scores of 0.95 and more are not uncommon), but keep in mind that the task given to the algorithms is quite difficult: for each document, it should pick the 5 subjects (out of nearly 28000) that best describe that document. There are numerous ways of getting this wrong and only a few ways of picking a good set of subjects. So even a rather low F1 score of 0.2 means that around one out of five subjects was right - much better than chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Union of top K concepts\n",
    "\n",
    "Let's try a variation of averaging the scores of the different algorithms. This time we only pick the top K subjects (where K is a small number, e.g. 2 or 3) suggested by each algorithm and use their union as the suggested concepts. We will still take the mean of scores in order to rank the results (otherwise the NDCG score would not be well defined as it requires the results to have some stable ranking order) but will drastically reduce the number of subjects being considered.\n",
    "\n",
    "This is a bit challenging to implement within the ranked suggestions format of pyltr, so what we will do instead is to implement it using sklearn multilabel prediction format and then convert that to a list of ranking predictions using the qids and cids information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             union-1:\tNDCG=0.3482\tF1@10=0.3087\n",
      "             union-2:\tNDCG=0.4570\tF1@10=0.3722\n",
      "             union-3:\tNDCG=0.5203\tF1@10=0.3812\n",
      "             union-5:\tNDCG=0.5928\tF1@10=0.3611\n",
      "            union-10:\tNDCG=0.6598\tF1@10=0.3633\n",
      "            union-20:\tNDCG=0.6455\tF1@10=0.3772\n",
      "            union-40:\tNDCG=0.6633\tF1@10=0.3896\n"
     ]
    }
   ],
   "source": [
    "def multilabel_to_ranking_preds(scores, qids, cids):\n",
    "    \"\"\"convert a sklearn multilabel prediction to pyltr ranking according to the given qids and cids\"\"\"\n",
    "    \n",
    "    qid_to_docid = collections.OrderedDict()\n",
    "    for qid in qids:\n",
    "        if qid not in qid_to_docid:\n",
    "            qid_to_docid[qid] = len(qid_to_docid)\n",
    "    return np.array([scores[qid_to_docid[qid],cid] for qid, cid in zip(qids, cids)])\n",
    "\n",
    "def select_top_k(scores, k):\n",
    "    mask = binarize_multilabel_pred(scores, k=k)\n",
    "    return scores * mask\n",
    "\n",
    "def union_of_top_k(scores, qids, cids, k):\n",
    "    top_scores = np.zeros(scores.shape)\n",
    "    for algoid in range(len(ALGORITHMS)):\n",
    "        top_scores[:,:,algoid] = select_top_k(scores[:,:,algoid], k=k)\n",
    "    \n",
    "    preds_union = multilabel_to_ranking_preds(top_scores, qids, cids)\n",
    "    name = \"union-{}\".format(k)\n",
    "    eval_results[name] = evaluate(name, Eqids, Ey, Etrue, preds_union.mean(1), Ecids)\n",
    "\n",
    "for k in (1,2,3,5,10,20,40):\n",
    "    union_of_top_k(Escores, Eqids, Ecids, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results were not encouraging. NDCG and F1 scores are generally lower than for the mean strategy. With higher K values, the results get closer to the ones for the mean method, which is expected since having a low limit on the number of results considered is the only aspect that differentiates this method from using just the mean of all scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization of prediction scores\n",
    "\n",
    "Each algorithm returns its scores on the scale 0.0 to 0.1, but they may be using different ranges within that scale. Because of this, simple averaging of scores may cause one algorithm which tends to report high scores to have much more impact on the result than another one that uses low scores. We can counteract that by normalizing the scores before taking the mean of scores. `scikit-learn` implements several normalization methods: L1, L2 and max-value. Let's see if they have an effect on merging results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             mean-l1:\tNDCG=0.6896\tF1@10=0.3967\n",
      "             mean-l2:\tNDCG=0.7014\tF1@10=0.4081\n",
      "            mean-max:\tNDCG=0.6809\tF1@10=0.3932\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def normalized_scores(scores, norm):\n",
    "    \"\"\"normalize the given predictions (in sklearn multilabel format)\n",
    "    using the given normalization method ('l1', 'l2' or 'max')\"\"\"\n",
    "\n",
    "for norm in ('l1', 'l2', 'max'):\n",
    "    # we will have to perform the normalization separately for each algorithm's output\n",
    "    Escores_norm = np.zeros(Escores.shape)\n",
    "    for algoid in range(len(ALGORITHMS)):\n",
    "        Escores_norm[:,:,algoid] = normalize(Escores[:,:,algoid], norm=norm)\n",
    "    Epreds_norm = multilabel_to_ranking_preds(Escores_norm, Eqids, Ecids)\n",
    "    eval_results[\"mean-\" + norm] = evaluate(\"mean-\" + norm, Eqids, Ey, Etrue, Epreds_norm.mean(1), Ecids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not seem to help much either. All normalization methods give similar results as the mean method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PAV aka Isotonic Regression\n",
    "\n",
    "The effectiveness of each algorithms varies by concept. Some concepts are given higher scores than they should, which results in false positives. Others are given too low scores, resulting in false negatives. We can try to correct some of this bias using a statistical method called isotonic regression, also known as Pool Adjacent Violators (PAV). The idea is to assess the scores given to individual concepts for a document against the likelihood that the concept was among the gold standard subjects of that document. The resulting models (separate models per concept and algorithm), initialized on training data, will then be able to convert raw scores returned by the algorithm to probabilities of the concept being correct. The resulting probabilities are then all on the same scale (0.0 to 0.1) which makes it easy to compare and combine them.\n",
    "\n",
    "One complication is that the frequency distribution of concepts in any training set is likely to be very skewed. Many concepts don't appear at all in training data. Others appear only a few times. We must decide when we have enough examples of a concept to be able to create an isotonic regression model.\n",
    "\n",
    "When applying the regression models on actual data, how to handle concepts for which we don't have a regression model is a problem. In this case we will simply use the raw, untransformed scores.\n",
    "\n",
    "We will try different variations of isotonic regression to see if they help improve the overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           mean-reg1:\tNDCG=0.7059\tF1@10=0.4197\n",
      "           mean-reg3:\tNDCG=0.7136\tF1@10=0.4292\n",
      "           mean-reg5:\tNDCG=0.7136\tF1@10=0.4283\n",
      "           mean-reg8:\tNDCG=0.7119\tF1@10=0.4240\n",
      "          mean-reg10:\tNDCG=0.7099\tF1@10=0.4225\n",
      "          mean-reg15:\tNDCG=0.7111\tF1@10=0.4206\n",
      "          mean-reg20:\tNDCG=0.7078\tF1@10=0.4173\n",
      "CPU times: user 43.9 s, sys: 2.96 s, total: 46.8 s\n",
      "Wall time: 46.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "def build_reg_models(k):\n",
    "    \"\"\"build a dictionary of regression models based on training documents. Models will only be created for concepts with a minimum of K true instances in the training data.\"\"\"\n",
    "\n",
    "    reg_models = {}\n",
    "    for cid in range(n_concepts):\n",
    "        if Ttrue[:,cid].sum() >= k:\n",
    "            for algoid in range(len(ALGORITHMS)):\n",
    "                reg = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "                reg.fit(Tscores[:,cid,algoid], Ttrue[:,cid])\n",
    "                reg_models[(algoid,cid)] = reg\n",
    "    return reg_models\n",
    "\n",
    "def reg_predict(reg_models, scores, algoid):\n",
    "    reg_scores = np.zeros(scores.shape)\n",
    "    for cid in range(n_concepts):\n",
    "        reg_model = reg_models.get((algoid, cid))\n",
    "        if reg_model is not None:\n",
    "            reg_scores[:,cid] = reg_model.predict(scores[:,cid])\n",
    "        else:\n",
    "            reg_scores[:,cid] = scores[:, cid] # use raw score\n",
    "    return reg_scores\n",
    "\n",
    "for k in (1,3,5,8,10,15,20):\n",
    "    reg_models = build_reg_models(k)\n",
    "    Escores_reg = np.zeros(Escores.shape)\n",
    "    for algoid in range(len(ALGORITHMS)):\n",
    "        Escores_reg[:,:,algoid] = reg_predict(reg_models, Escores[:,:,algoid], algoid)\n",
    "    Epreds_reg = multilabel_to_ranking_preds(Escores_reg, Eqids, Ecids)\n",
    "    name = \"mean-reg{}\".format(k)\n",
    "    eval_results[name] = evaluate(name, Eqids, Ey, Etrue, Epreds_reg.mean(1), Ecids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are somewhat better than anything seen so far, indicating that isotonic regression can help improve the quality of fusion. There appears to be a sweet spot somewhere around 3-5 occurrences of concepts as the threshold value for when to apply isotonic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning to Rank\n",
    "\n",
    "Let's try a true machine learning based fusion approach. We will apply the state of the art Learning to Rank algorithm LambdaMART, which is typically used for ranking results in a search engine. In this case, we consider each document a \"query\" and the predicted subjects as \"results\" and will try to come up with the ideal ranking of those subjects so that gold standard subjects are ranked as high as possible while non-relevant subjects are ranked lower. The algorithm is given a ranking measure that it then attempts to optimize; we will use NDCG as the measure to optimize.\n",
    "\n",
    "The LambdaMART algorithm is conveniently implemented in the `pyltr` library. This implementation has some advanced features: it can be connected to a monitor that periodically evaluates the learned model on validation data and keeps track of the learning progress. If the monitor notices that the learning has reached a plateau - no improvement in validation scores has been made in the last K training rounds - it can stop the learning as well as roll back the model to the state when it reached that plateau. This is called *early stopping* and *trimming*, respectively, and it should help guard against overfitting the model on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "\n",
    "Any Learning to Rank algorithm requires features for the \"results\" (i.e. subjects in our case); here we will simply use the raw scores predicted by the subject indexing algorithms as the features. In addition, we will use the lengths of the original input documents as features, as well as some concept features such as label length. These are all quite easy for us to generate, but we don't know whether the LTR algorithm has any use for them - let's give it whatever features we can think of and let it make its own decisions on whether to use them and how.\n",
    "\n",
    "First the document-level features (currently just size):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read information about document lengths from a data file generated using the `du -b` command\n",
    "doc_length = {}\n",
    "with open(FILE_SIZES) as file_sizes:\n",
    "    for line in file_sizes:\n",
    "        size, filename = line.strip().split()\n",
    "        basename = filename[:-4].split('-')[-1]\n",
    "        doc_length[int(basename)] = int(size)\n",
    "\n",
    "# define a function that looks up document lengths based on qids and returns a 1D feature matrix\n",
    "def doc_features(qids):\n",
    "    \"\"\"return a NumPy array of document features (in practice, file sizes) corresponding to the given qids\"\"\"\n",
    "    \n",
    "    return np.array([np.array([doc_length[qid]]) for qid in qids])\n",
    "\n",
    "# let's also make note of what document features we used, for later display\n",
    "DOCUMENT_FEATURES = ('d-length',)\n",
    "\n",
    "# generate document features for train, validate and test sets\n",
    "Tdocf = doc_features(Tqids)\n",
    "Vdocf = doc_features(Vqids)\n",
    "Edocf = doc_features(Eqids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we generate some concept level features. We will generate several features per concept, based mainly on concept labels: label length (characters and words), number and proportion of capital letters etc. Also we will count the number of occurrences of each concept in the train set and use that as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the per-concept frequencies from the validation(!) set into a 1D NumPy array with size n_concepts\n",
    "\n",
    "# To (try to) avoid overfitting, we can use just the base 10 or natural logarithms of the frequencies (incremented by\n",
    "# one to avoid divide by zero) and round them to the nearest integer to indicate rough magnitude\n",
    "concept_freq = np.log(Vtrue.sum(axis=0) + 1).round()    # natural logarithm\n",
    "#concept_freq = np.log10(Vtrue.sum(axis=0) + 1).round()  # base 10 logarithm\n",
    "#concept_freq = Vtrue.sum(axis=0)                         # raw frequency value\n",
    "concept_freq_finna = np.log(np.array(subject_freq_finna) + 1).round()\n",
    "\n",
    "def features_of_concept(cid):\n",
    "    \"\"\"return a 1D NumPy array of features for a particular concept\"\"\"\n",
    "    \n",
    "    features = []\n",
    "#    features.append(concept_freq[cid])  # frequency in the validate set\n",
    "#    features.append(concept_freq_finna[cid])  # frequency in the Finna.fi training corpus\n",
    "    label = subject_label[cid]\n",
    "    features.append(len(label))  # label length in characters\n",
    "    features.append(len(label.split()))  # label length in words\n",
    "    caps = sum(1 for c in label if c.isupper())\n",
    "    features.append(caps)  # number of capital letters\n",
    "    features.append(caps / len(label))  # proportion of capital letters\n",
    "    return np.array(features)\n",
    "\n",
    "def concept_features(cids):\n",
    "    \"\"\"return a 2D NumPy array of features for the given concepts\"\"\"\n",
    "    \n",
    "    return np.array([features_of_concept(cid) for cid in cids])\n",
    "\n",
    "# let's also make note of what concept features we used, for later display\n",
    "CONCEPT_FEATURES = (\n",
    "    #'c-freq', \n",
    "    #'c-finnafreq',\n",
    "    'c-chars', 'c-words', 'c-caps', 'c-capprop')\n",
    "\n",
    "# generate concept features for train, validate and test sets\n",
    "Tconcf = concept_features(Tcids)\n",
    "Vconcf = concept_features(Vcids)\n",
    "Econcf = concept_features(Ecids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can combine the features into matrices for feeding into the LTR algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(428445, 8)\n",
      "(92779, 8)\n",
      "(91044, 8)\n"
     ]
    }
   ],
   "source": [
    "TXall = np.hstack((TX, Tdocf, Tconcf))\n",
    "print(TXall.shape)\n",
    "\n",
    "VXall = np.hstack((VX, Vdocf, Vconcf))\n",
    "print(VXall.shape)\n",
    "\n",
    "EXall = np.hstack((EX, Edocf, Econcf))\n",
    "print(EXall.shape)\n",
    "\n",
    "# make note what features we used\n",
    "ALL_FEATURES = ALGORITHMS + DOCUMENT_FEATURES + CONCEPT_FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can run the LTR algorithm itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter  Train score  OOB Improve    Remaining                           Monitor Output \n",
      "    1       0.6268       0.6205      109.66m      C:      0.6207 B:      0.6207 S:  0\n",
      "    2       0.6494       0.0262      114.11m      C:      0.6475 B:      0.6475 S:  0\n",
      "    3       0.6550       0.0073      115.42m      C:      0.6569 B:      0.6569 S:  0\n",
      "    4       0.6816       0.0197      115.96m      C:      0.6789 B:      0.6789 S:  0\n",
      "    5       0.7006       0.0041      116.64m      C:      0.6813 B:      0.6813 S:  0\n",
      "    6       0.6874       0.0125      116.79m      C:      0.6942 B:      0.6942 S:  0\n",
      "    7       0.7113       0.0060      117.00m      C:      0.6987 B:      0.6987 S:  0\n",
      "    8       0.7023       0.0096      117.21m      C:      0.7114 B:      0.7114 S:  0\n",
      "    9       0.7090       0.0044      117.19m      C:      0.7151 B:      0.7151 S:  0\n",
      "   10       0.7341       0.0038      117.31m      C:      0.7189 B:      0.7189 S:  0\n",
      "   15       0.7398       0.0017      117.72m      C:      0.7346 B:      0.7346 S:  0\n",
      "   20       0.7514       0.0012      117.48m      C:      0.7400 B:      0.7400 S:  0\n",
      "   25       0.7531       0.0009      117.22m      C:      0.7462 B:      0.7464 S:  1\n",
      "   30       0.7408       0.0006      116.76m      C:      0.7488 B:      0.7488 S:  0\n",
      "   35       0.7527      -0.0001      116.43m      C:      0.7480 B:      0.7492 S:  2\n",
      "   40       0.7454      -0.0004      115.85m      C:      0.7492 B:      0.7492 S:  0\n",
      "   45       0.7382       0.0001      115.40m      C:      0.7494 B:      0.7497 S:  1\n",
      "   50       0.7427      -0.0003      114.85m      C:      0.7489 B:      0.7500 S:  4\n",
      "   60       0.7557      -0.0000      113.81m      C:      0.7487 B:      0.7500 S: 14\n",
      "   70       0.7686      -0.0003      112.62m      C:      0.7490 B:      0.7500 S: 24\n",
      "   80       0.7578      -0.0006      111.46m      C:      0.7476 B:      0.7500 S: 34\n",
      "   90       0.7638      -0.0006      110.61m      C:      0.7476 B:      0.7500 S: 44\n",
      "Early termination at iteration  95\n",
      "CPU times: user 11min 41s, sys: 64 ms, total: 11min 41s\n",
      "Wall time: 11min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create a monitor for early stopping and trimming, using validation data\n",
    "monitor = pyltr.models.monitors.ValidationMonitor(\n",
    "    VXall, Vy, Vqids, metric=ndcg_metric, stop_after=50)\n",
    "\n",
    "# create a LambdaMART model for learning a suitable ranking\n",
    "model = pyltr.models.LambdaMART(\n",
    "    metric=ndcg_metric,\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.2,\n",
    "    query_subsample=0.1,\n",
    "    max_leaf_nodes=10,\n",
    "    min_samples_leaf=64,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# fit the model to training data\n",
    "model.fit(TXall, Ty, Tqids, monitor=monitor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a model trained on the training documents and validated on the validation documents. First, since we spent so much time creating the model, let's save it for later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ltr-model.joblib']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the learned model for later use\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(model, 'ltr-model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the model to predict subject rankings on the evaluation documents and evaluate how well it did compared to the other fusion approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 ltr:\tNDCG=0.7152\tF1@10=0.4345\n"
     ]
    }
   ],
   "source": [
    "Epred = model.predict(EX)\n",
    "eval_results[\"ltr\"] = evaluate(\"ltr\", Eqids, Ey, Etrue, Epred, Ecids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LTR results for test set\n",
    "\n",
    "### kirjastonhoitaja dataset with old, bad Maui scores\n",
    "\n",
    "#### using only raw scores as features (3 features)\n",
    "* max_pred=25, n_estimators=1000, query_subsample=0.5, max_leaf_nodes=10, min_samples_leaf=64: \n",
    "  * after 52(+50) iterations: NDCG=0.6097\tF1=0.3108\n",
    "* max_pred=50, n_estimators=1000, query_subsample=0.5, max_leaf_nodes=10, min_samples_leaf=64: \n",
    "  * after 98(+30) iterations: NDCG=0.5745\tF1=0.3056\n",
    "* max_pred=50, n_estimators=1000, query_subsample=0.25, max_leaf_nodes=10, min_samples_leaf=64: \n",
    "  * after 90(+30) iterations: NDCG=0.5828\tF1=0.3135\n",
    "* max_pred=50, n_estimators=1000, query_subsample=0.1, max_leaf_nodes=10, min_samples_leaf=64: \n",
    "  * after 54(+30) iterations: NDCG=0.5825\tF1=0.3053 (6min 15s)\n",
    "\n",
    "#### using raw score + doc length + concept label length&caps (=8 features)\n",
    "\n",
    "* max_pred=50, n_estimators=1000, query_subsample=0.1, max_leaf_nodes=10, min_samples_leaf=64: \n",
    "  * after 78(+30) iterations: NDCG=0.5848\tF1=0.3076 (8min 28s)\n",
    "* max_pred=50, n_estimators=1000, learning_rate=0.2, query_subsample=0.1, max_leaf_nodes=10, min_samples_leaf=64: \n",
    "  * after 66(+50) iterations: NDCG=0.5715\tF1=0.3039 (8min 54s)\n",
    "\n",
    "### kirjastonhoitaja dataset with new Maui scores\n",
    "\n",
    "#### using 8 features\n",
    "\n",
    "* max_pred=50, n_estimators=1000, learning_rate=0.2, query_subsample=0.1, max_leaf_nodes=10, min_samples_leaf=64: \n",
    "  * after 40(+50) iterations: NDCG=0.5925\tF1=0.3159 (7min 13s)\n",
    "  \n",
    "#### using 9 features (all above + train concept freq)\n",
    "\n",
    "* max_pred=50, n_estimators=1000, learning_rate=0.2, query_subsample=0.1, max_leaf_nodes=10, min_samples_leaf=64: \n",
    "  * after 135(+50) iterations: NDCG=0.3777\tF1=0.0069 (15min 37s) ??? **what went wrong?** Apparently the model got too hooked on c-freq (importance 0.2477 vs. Maui 0.3144) and this didn't generalize so well...\n",
    "  \n",
    "#### using 9 features (8 basic + log10 of train concept freq)\n",
    "\n",
    "* max_pred=50, n_estimators=1000, learning_rate=0.2, query_subsample=0.1, max_leaf_nodes=10, min_samples_leaf=64:\n",
    "  * after 58(+50) iterations: NDCG=0.5722\tF1=0.2821 (9min 21s) -- **better than raw freq but still not good**\n",
    "\n",
    "#### using 9 features (8 basic + logN of train concept freq)\n",
    "* max_pred=50, n_estimators=1000, learning_rate=0.2, query_subsample=0.1, max_leaf_nodes=10, min_samples_leaf=64:\n",
    "  * after 69(+50) iterations: NDCG=0.3641\tF1=0.0000 (9min 11s) -- **horrible**\n",
    "  \n",
    "#### using 9 features (8 basic + logN of validate concept freq)\n",
    "* max_pred=50, n_estimators=1000, learning_rate=0.2, query_subsample=0.1, max_leaf_nodes=10, min_samples_leaf=64:\n",
    "  * after 57(+50) iterations: NDCG=0.5973\tF1=0.3234 (8min 52s) -- **new record**\n",
    "  \n",
    "#### using 9 features (8 basic + log10 of validate concept freq)\n",
    "* max_pred=50, n_estimators=1000, learning_rate=0.2, query_subsample=0.1, max_leaf_nodes=10, min_samples_leaf=64:\n",
    "  * after 72(+50) iterations: NDCG=0.5984\tF1=0.3147 (9min 46s)\n",
    "\n",
    "#### using 9 features (8 basic + validate concept freq)\n",
    "* max_pred=50, n_estimators=1000, learning_rate=0.2, query_subsample=0.1, max_leaf_nodes=10, min_samples_leaf=64:\n",
    "  * after 35(+50) iterations: NDCG=0.5952\tF1=0.3199 (6min 44s)\n",
    "\n",
    "#### using 9 features (8 basic + finna concept freq)\n",
    "\n",
    "* max_pred=50, n_estimators=1000, learning_rate=0.2, query_subsample=0.1, max_leaf_nodes=10, min_samples_leaf=64:\n",
    "  * after 103(+50) iterations: NDCG=0.5728\tF1=0.2566 (c-finnafreq importance: 0.1774)\n",
    "\n",
    "#### using 9 features (8 basic + logN of finna concept freq)\n",
    "\n",
    "* max_pred=50, n_estimators=1000, learning_rate=0.2, query_subsample=0.1, max_leaf_nodes=10, min_samples_leaf=64:\n",
    "  * after 117(+50) iterations: NDCG=0.5731\tF1=0.2955 (c-finnafreq importance: 0.0340)\n",
    "  \n",
    "#### using 10 features (8 basic + validate concept freq + finna concept freq)\n",
    "\n",
    "* max_pred=50, n_estimators=1000, learning_rate=0.2, query_subsample=0.1, max_leaf_nodes=10, min_samples_leaf=64:\n",
    "  * after 79(+50) iterations: NDCG=0.5936\tF1=0.3189 (11min 37s)\n",
    "  \n",
    "### jyu-theses dataset\n",
    "\n",
    "#### using 8 features\n",
    "\n",
    "* max_pred=50, n_estimators=1000, learning_rate=0.2, query_subsample=0.1, max_leaf_nodes=10, min_samples_leaf=64: \n",
    "  * after 95(+50) iterations: NDCG=0.7152\tF1@10=0.4345 (11min 41s)\n",
    "\n",
    "#### using 9 features (8 basic + logN of validate concept freq)\n",
    "* max_pred=50, n_estimators=1000, learning_rate=0.2, query_subsample=0.1, max_leaf_nodes=10, min_samples_leaf=64:\n",
    "  * after 85(+50) iterations: NDCG=0.7164\tF1@10=0.4403 (16min 37s)\n",
    "  \n",
    "#### using 9 features (8 basic + validate concept freq)\n",
    "* max_pred=50, n_estimators=1000, learning_rate=0.2, query_subsample=0.1, max_leaf_nodes=10, min_samples_leaf=64:\n",
    "  * after 161(+50) iterations: NDCG=0.7101\tF1@10=0.4286 (25min 40s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check how much the input from each algorithm contributed to the rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               tfidf importance: 0.1574\n",
      "            fasttext importance: 0.2074\n",
      "                maui importance: 0.4844\n",
      "            d-length importance: 0.0567\n",
      "             c-chars importance: 0.0617\n",
      "             c-words importance: 0.0324\n",
      "              c-caps importance: 0.0000\n",
      "           c-capprop importance: 0.0000\n"
     ]
    }
   ],
   "source": [
    "for feature, importance in zip(ALL_FEATURES, model.feature_importances_):\n",
    "    print(\"{:>20} importance: {:.4f}\".format(feature, importance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: plot the results using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

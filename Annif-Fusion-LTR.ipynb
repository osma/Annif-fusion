{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annif Fusion experiment\n",
    "\n",
    "**Osma Suominen, October 2018**\n",
    "\n",
    "This notebook is an experiment for evaluating different methods to combine results of multiple subject indexing algorithms. We will use a document collection where gold standard subjects have been manually assigned and compare the subjects assigned by three different algorithm first individually, then in combinations created by different fusion approaches. In particular, we will test simple mean-of-subject-scores and union-of-top-K approaches as well as more advanced methods such as score normalization, isotonic regression and Learning to Rank style machine learning. Ideally, we would like to find a method for combining results from multiple algorithm that gives us the best quality results, combining the strengths of individual algorithms (both statistical/associative and lexical) while eliminating the effects of their weaknesses. The findings will then inform the further development of Annif.\n",
    "\n",
    "The experiment was inspired by the Martin Toepfer's paper [Fusion architectures for automatic subject indexing under concept drift](https://link.springer.com/article/10.1007/s00799-018-0240-3) as well as discussions with him during and after the NKOS2018 workshop.\n",
    "\n",
    "If you want to run this yourself, you need Python 3.5+ with the following libraries:\n",
    "\n",
    "* jupyter (obviously)\n",
    "* scikit-learn\n",
    "* pyltr\n",
    "* matplotlib (for the final plots)\n",
    "\n",
    "This document is (c) Osma Suominen. It may be shared and reused according to the terms of the [CC Attribution 4.0 International](https://creativecommons.org/licenses/by/4.0/) license. Attribution should include the name of the author and a link to the original source document. However, code snippets in this notebook may be freely reused according to the [CC0 1.0 Universal](https://creativecommons.org/publicdomain/zero/1.0/) license. Attribution for code is requested but not required.\n",
    "\n",
    "## Document corpus\n",
    "\n",
    "We will use the [\"Ask a Librarian\" document corpus](https://github.com/NatLibFi/Annif-corpora/tree/master/fulltext/kirjastonhoitaja) for the experiment. It contains 3150 short question-answer pairs in Finnish language. The collection is subdivided into three subsets: train (n=2625), validate (n=213) and test (n=312). All the final evaluations will be performed on the test set, which contains the most recent questions asked during the year 2017. However, for some of the fusion methods we will make use of the train and validate subsets in order to fine-tune the way results are combined.\n",
    "\n",
    "All documents in the collection have been assigned gold standard subjects by librarians, stored in `*.tsv` files where the basenames correspond to the original `*.txt` files (which are not read by this code at all and are not included in this repository). Librarians have assigned at least 4 subjects per document; the average is 4.4-4.8 subjects per document, depending on the subset.\n",
    "\n",
    "In addition, the documents have been analyzed using the [Annif](https://github.com/NatLibFi/Annif/) tool (development version v0.31.0) using three independent automated subject indexing algorithms: TF-IDF vector similarity, fastText and Maui, asking each algorithm to suggest up to 1000 subjects per document (however, we will in practice only load a fraction of these) with scores ranging from 1.0 to 0.0 given to each suggested subject. The subjects suggested by these algoritms are stored, respectively, in `*.tfidf`, `*.fasttext` and `*.maui` files.\n",
    "\n",
    "First we load some basic modules and define the locations of the data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyltr\n",
    "import os\n",
    "import os.path\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "SUBJECTFILE='data/yso.tsv'\n",
    "TRAIN_DIR='data/kirjastonhoitaja/train/'\n",
    "VALI_DIR='data/kirjastonhoitaja/validate/'\n",
    "TEST_DIR='data/kirjastonhoitaja/test/'\n",
    "FILE_SIZES='data/kirjastonhoitaja/file-sizes.txt'\n",
    "ALGORITHMS = ('tfidf','fasttext','maui')  # these correspond to file extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject vocabulary\n",
    "\n",
    "All the subjects have been chosen from the Finnish General Ontology YSO, which contains around 28000 concepts.\n",
    "Here we load the vocabulary from a TSV file where the first column is the concept URI and the second column is the concept label in Finnish. This has been extracted from the full YSO SKOS file.\n",
    "\n",
    "Hereafter we will only use integer concept IDs which range from 0 to (n_concepts-1). All calculations are performed using the concept IDs so we just need to map the concept URIs which appear in files to their IDs.\n",
    "\n",
    "In addition, we store the concept labels for later use as features for the Learning to Rank algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary loaded with 27760 concepts\n"
     ]
    }
   ],
   "source": [
    "uri_to_cid = {}\n",
    "subject_label = {}\n",
    "with open(SUBJECTFILE) as subjf:\n",
    "    for cid, line in enumerate(subjf):\n",
    "        uri, label = line.strip().split(\"\\t\")\n",
    "        uri_to_cid[uri] = cid\n",
    "        subject_label[cid] = label\n",
    "n_concepts = len(uri_to_cid)\n",
    "print(\"Vocabulary loaded with\", n_concepts, \"concepts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document corpus\n",
    "\n",
    "We define a function to load the data from a directory of files in the format explained above. To be able to use both `scikit-learn` and `pyltr` libraries, we will need to express the data in two somewhat different formats, both based on multidimensional NumPy arrays:\n",
    "    \n",
    "1. pyltr ranking format, where the NumPy array rows correspond to document-subject pairs, i.e. there are multiple rows per document, one row per subject that has been suggested by at least one algorithm. A separate qids array (\"query IDs\", actually document IDs when applied this way) maps the document-subject pairs to the individual documents.\n",
    "2. scikit-learn multilabel format, where the NumPy array has one row per document and the columns are individual concepts (either True/False for the gold standard subjects, or subject scores for the predicted subjects)\n",
    "\n",
    "We will define this function once and then use it to parse all three document subsets (train, validate, test). We will only load up to 25 suggested subjects per document to keep the size of the rankings manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TX shape: (283089, 3)\n",
      "Ty shape: (283089,)\n",
      "Tqids shape: (283089,)\n",
      "Tcids shape: (283089,)\n",
      "Ttrue shape: (2625, 27760)\n",
      "Tscores shape: (2625, 27760, 3)\n",
      "\n",
      "VX shape: (22604, 3)\n",
      "Vy shape: (22604,)\n",
      "Vqids shape: (22604,)\n",
      "Vcids shape: (22604,)\n",
      "Vtrue shape: (213, 27760)\n",
      "Vscores shape: (213, 27760, 3)\n",
      "\n",
      "EX shape: (33318, 3)\n",
      "Ey shape: (33318,)\n",
      "Eqids shape: (33318,)\n",
      "Ecids shape: (33318,)\n",
      "Etrue shape: (312, 27760)\n",
      "Escores shape: (312, 27760, 3)\n"
     ]
    }
   ],
   "source": [
    "def load_data(directory, max_pred=50):\n",
    "    \"\"\"Reads document corpus with gold standard and predicted subjects for each document.\n",
    "    Only the top max_pred predictions per document and algorithm are considered for ranking.\n",
    "    Returns a tuple with\n",
    "     - predictions of individual algorithms as a 2D NumPy array with the shape (doc-suggestions, n_algos)\n",
    "     - relevant (gold standard) concepts as a 1D NumPy array with the shape (doc-suggestions)\n",
    "     - qids (actually document ids) as a 1D NumPy array with length doc-suggestions\n",
    "     - concept IDs of suggested concepts as a 1D NumPy array with shape (doc-suggestions)\n",
    "     - gold standard subjects in sklearn multilabel format as a 2D NumPy array with shape (n_docs, n_concepts) \n",
    "     - predictions of individual algorithms in sklearn multilabel format as a 3D NumPy array with shape (n_docs, n_concepts, n_algos)\n",
    "    \"\"\"\n",
    "    \n",
    "    # temporary lists to be converted into NumPy arrays at the end\n",
    "    docdatalist = []\n",
    "    docscorelist = []\n",
    "    docgoldlist = []\n",
    "    doctruelist = []\n",
    "    qidlist = []\n",
    "    cidlist = []\n",
    "    \n",
    "    for tsvfilename in glob.glob(os.path.join(directory, '*.tsv')):\n",
    "        basename = tsvfilename[:-4]\n",
    "        \n",
    "        # Initialize binary mask containing the top max_pred concepts that were suggested by any algorithm.\n",
    "        # This will be used to pare down the doc-suggestions for ranking so that we will skip concepts\n",
    "        # that were not suggested by any algorithm. They will still be considered in F1 score evaluation,\n",
    "        # which is not based only on the ranked suggestions but used the full gold standard subjects.\n",
    "        mask = np.zeros(n_concepts, dtype=np.bool)\n",
    "\n",
    "        docdata = np.zeros((n_concepts, len(ALGORITHMS)))\n",
    "        for algoid, fileext in enumerate(ALGORITHMS):\n",
    "            with open(basename + \".\" + fileext) as algooutput:\n",
    "                lines = 0\n",
    "                for lineno, line in enumerate(algooutput):\n",
    "                    uri, _, score = line.strip().split(\"\\t\")\n",
    "                    if uri not in uri_to_cid:\n",
    "                        continue  # ignore unknown URIs\n",
    "                    cid = uri_to_cid[uri]\n",
    "                    docdata[cid,algoid] = float(score)\n",
    "                    if lineno < max_pred:\n",
    "                        mask[cid] = True\n",
    "        docdatalist.append(docdata[mask])\n",
    "        docscorelist.append(docdata)\n",
    "    \n",
    "        docgold = np.zeros(n_concepts, dtype=np.bool)\n",
    "        with open(tsvfilename) as tsvfile:\n",
    "            for line in tsvfile:\n",
    "                uri = line.split(\"\\t\")[0]\n",
    "                if uri in uri_to_cid:\n",
    "                    cid = uri_to_cid[uri]\n",
    "                    docgold[cid] = True\n",
    "        docgoldlist.append(docgold[mask])\n",
    "        doctruelist.append(docgold)\n",
    "        \n",
    "        fileid = int(basename.split('/')[-1].split('-')[-1])\n",
    "        qidlist.append(np.full(mask.sum(), fileid))\n",
    "        cidlist.append(np.arange(n_concepts)[mask])\n",
    "\n",
    "    return np.concatenate(docdatalist), np.concatenate(docgoldlist), np.concatenate(qidlist), \\\n",
    "           np.concatenate(cidlist), np.array(doctruelist), np.array(docscorelist)\n",
    "\n",
    "# now load the train, validate and test documents\n",
    "TX, Ty, Tqids, Tcids, Ttrue, Tscores = load_data(TRAIN_DIR)\n",
    "VX, Vy, Vqids, Vcids, Vtrue, Vscores = load_data(VALI_DIR)\n",
    "EX, Ey, Eqids, Ecids, Etrue, Escores = load_data(TEST_DIR)\n",
    "\n",
    "# Show some information about the shapes\n",
    "print(\"TX shape:\", TX.shape)\n",
    "print(\"Ty shape:\", Ty.shape)\n",
    "print(\"Tqids shape:\", Tqids.shape)\n",
    "print(\"Tcids shape:\", Tcids.shape)\n",
    "print(\"Ttrue shape:\", Ttrue.shape)\n",
    "print(\"Tscores shape:\", Tscores.shape)\n",
    "print()\n",
    "print(\"VX shape:\", VX.shape)\n",
    "print(\"Vy shape:\", Vy.shape)\n",
    "print(\"Vqids shape:\", Vqids.shape)\n",
    "print(\"Vcids shape:\", Vcids.shape)\n",
    "print(\"Vtrue shape:\", Vtrue.shape)\n",
    "print(\"Vscores shape:\", Vscores.shape)\n",
    "print()\n",
    "print(\"EX shape:\", EX.shape)\n",
    "print(\"Ey shape:\", Ey.shape)\n",
    "print(\"Eqids shape:\", Eqids.shape)\n",
    "print(\"Ecids shape:\", Ecids.shape)\n",
    "print(\"Etrue shape:\", Etrue.shape)\n",
    "print(\"Escores shape:\", Escores.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics\n",
    "\n",
    "For evaluating the quality of results, we will use two metrics: [NDCG](https://en.wikipedia.org/wiki/Discounted_cumulative_gain#Normalized_DCG) which is based on ranked suggestions (if the top ranked suggestion is correct it will contribute more to the score than getting the 2nd or 3rd suggestion right) and [F1 score](https://en.wikipedia.org/wiki/F1_score) that only considers binary choices: either a concept is a subject of a document or it is not, there are no fuzzy options. \n",
    "\n",
    "For the **NDCG score**, we can simply use the implementation from the `pyltr` library. Here we limit the NDCG evaluation to the top 20 most highly ranked subjects (i.e. NDCG@20), meaning that subjects ranked 21st, 22nd etc won't contribute to the score (their effect would be quite small anyway do to the discounting performed in NDCG calculations).\n",
    "\n",
    "For the **F1 score**, we can use the `scikit-learn` implementation. However, its usage is complicated by the fact that we are mostly dealing with ranked suggestions in the pyltr format, so we need to convert those into the multilabel prediction format understood by scikit-learn and further binarize the predictions using a thresholding strategy. For simplicity we will pick the top 5 suggested concepts, so we get an amount of subjects that is similar to the librarian-assigned ones (around 4.5 subjects per document), and compare those to the gold standard. The score for each document will be calculated individually and the final result will be the average of those scores (i.e. sample based average).\n",
    "\n",
    "Finally, we will define an evaluation function that is given ranked suggestions as well as gold standard subjects (in both pyltr and scikit-learn formats) and some auxiliary information (qids and cids). It will calculate both NDCG and F1 score, print them out, and also return the scores so that they can be stored for later plotting etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# initialize the NDCG metric implementation\n",
    "ndcg_metric = pyltr.metrics.NDCG(k=20)\n",
    "\n",
    "def ranking_to_multilabel(pred, qids, cids):\n",
    "    \"\"\"convert from pyltr ranking prediction format to sklearn multilabel prediction format\"\"\"\n",
    "    doc_labels = collections.OrderedDict()  # key: qid, val: NumPy 1D array with length n_concepts\n",
    "    for score, qid, cid in zip(pred, qids, cids):\n",
    "        if qid not in doc_labels:\n",
    "            doc_labels[qid] = np.zeros(n_concepts)\n",
    "        doc_labels[qid][cid] = score\n",
    "    return np.array(list(doc_labels.values()))\n",
    "\n",
    "def binarize_multilabel_pred(pred, k=5):\n",
    "    \"\"\"convert predictions with scores to a boolean matrix, taking the top K predictions per document\"\"\"\n",
    "    # I'm pretty sure this could be done easier, perhaps in a single NumPy operation, but I can't figure it out\n",
    "    # Using a loop instead, it's not going to take long anyway\n",
    "    binary_labels = np.zeros(pred.shape, dtype=np.bool)\n",
    "    pred_order = pred.argsort()[:,::-1]\n",
    "    for i, mask in enumerate(pred_order[:,:k]):\n",
    "        binary_labels[i,mask] = True\n",
    "    return binary_labels & (pred > 0.0)  # make sure not to include predictions with 0.0 score\n",
    "\n",
    "def evaluate(name, qids, y, y_true, pred, cids):\n",
    "    \"\"\"evaluate a ranking, calculating its NDCG and F1 score. Print and return the scores\"\"\"\n",
    "    # calculate NDCG score\n",
    "    ndcg = ndcg_metric.calc_mean(qids, y, pred)\n",
    "\n",
    "    # calculate F1 score\n",
    "    pred_labels = ranking_to_multilabel(pred, qids, cids)\n",
    "    f1 = f1_score(y_true, binarize_multilabel_pred(pred_labels), average='samples')\n",
    "    \n",
    "    print(\"{:>20}:\\tNDCG={:.4f}\\tF1={:.4f}\".format(name, ndcg, f1))\n",
    "    return ndcg, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline evaluation\n",
    "\n",
    "Now we are ready for some baseline evaluations: we can evaluate the individual algorithms as well as a simplistic combination method using the mean of assigned scores (the only one implemented in Annif v0.31.0). We will store the results for later too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               tfidf:\tNDCG=0.4223\tF1=0.2220\n",
      "            fasttext:\tNDCG=0.2737\tF1=0.1329\n",
      "                maui:\tNDCG=0.5133\tF1=0.2949\n",
      "                mean:\tNDCG=0.5701\tF1=0.3142\n"
     ]
    }
   ],
   "source": [
    "# initialize dictionary for collecting evaluation results\n",
    "eval_results = collections.OrderedDict()\n",
    "\n",
    "# evaluate the individual algorithms\n",
    "for algoid, name in enumerate(ALGORITHMS):\n",
    "    eval_results[name] = evaluate(name, Eqids, Ey, Etrue, EX[:,algoid], Ecids)\n",
    "\n",
    "# evaluate the combination of algorithms using mean of scores\n",
    "eval_results[\"mean\"] = evaluate(\"mean\", Eqids, Ey, Etrue, EX.mean(1), Ecids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that out of individual algorithms, Maui performed best, followed by TF-IDF and fastText. The mean of scores gave better results than any algorithm alone. The NDCG and F1 measures seem to agree on which methods are better than others, as they both ranked the methods in the same order.\n",
    "\n",
    "Note that the F1 scores may seem quite low (in some classification tasks F1 scores of 0.95 and more are not uncommon), but keep in mind that the task given to the algorithms is quite difficult: for each document, it should pick the 5 subjects (out of nearly 28000) that best describe that document. There are numerous ways of getting this wrong and only a few ways of picking a good set of subjects. So even a rather low F1 score of 0.2 means that around one out of five subjects was right - much better than chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Union of top K concepts\n",
    "\n",
    "Let's try a variation of averaging the scores of the different algorithms. This time we only pick the top K subjects (where K is a small number, e.g. 2 or 3) suggested by each algorithm and use their union as the suggested concepts. We will still take the mean of scores in order to rank the results (otherwise the NDCG score would not be well defined as it requires the results to have some stable ranking order) but will drastically reduce the number of subjects being considered.\n",
    "\n",
    "This is a bit challenging to implement within the ranked suggestions format of pyltr, so what we will do instead is to implement it using sklearn multilabel prediction format and then convert that to a list of ranking predictions using the qids and cids information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             union-1:\tNDCG=0.3026\tF1=0.2289\n",
      "             union-2:\tNDCG=0.3988\tF1=0.2672\n",
      "             union-3:\tNDCG=0.4410\tF1=0.2773\n",
      "             union-5:\tNDCG=0.5091\tF1=0.2947\n",
      "            union-10:\tNDCG=0.5588\tF1=0.3036\n",
      "            union-20:\tNDCG=0.5560\tF1=0.3053\n",
      "            union-40:\tNDCG=0.5635\tF1=0.3096\n"
     ]
    }
   ],
   "source": [
    "def multilabel_to_ranking_preds(scores, qids, cids):\n",
    "    \"\"\"convert a sklearn multilabel prediction to pyltr ranking according to the given qids and cids\"\"\"\n",
    "    \n",
    "    qid_to_docid = collections.OrderedDict()\n",
    "    for qid in qids:\n",
    "        if qid not in qid_to_docid:\n",
    "            qid_to_docid[qid] = len(qid_to_docid)\n",
    "    return np.array([scores[qid_to_docid[qid],cid] for qid, cid in zip(qids, cids)])\n",
    "\n",
    "def select_top_k(scores, k):\n",
    "    mask = binarize_multilabel_pred(scores, k=k)\n",
    "    return scores * mask\n",
    "\n",
    "def union_of_top_k(scores, qids, cids, k):\n",
    "    top_scores = np.zeros(scores.shape)\n",
    "    for algoid in range(len(ALGORITHMS)):\n",
    "        top_scores[:,:,algoid] = select_top_k(scores[:,:,algoid], k=k)\n",
    "    \n",
    "    preds_union = multilabel_to_ranking_preds(top_scores, qids, cids)\n",
    "    name = \"union-{}\".format(k)\n",
    "    eval_results[name] = evaluate(name, Eqids, Ey, Etrue, preds_union.mean(1), Ecids)\n",
    "\n",
    "for k in (1,2,3,5,10,20,40):\n",
    "    union_of_top_k(Escores, Eqids, Ecids, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results were not encouraging. NDCG and F1 scores are generally lower than for the mean strategy. With higher K values, the results get closer to the ones for the mean method, which is expected since having a low limit on the number of results considered is the only aspect that differentiates this method from using just the mean of all scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization of prediction scores\n",
    "\n",
    "Each algorithm returns its scores on the scale 0.0 to 0.1, but they may be using different ranges within that scale. Because of this, simple averaging of scores may cause one algorithm which tends to report high scores to have much more impact on the result than another one that uses low scores. We can counteract that by normalizing the scores before taking the mean of scores. `scikit-learn` implements several normalization methods: L1, L2 and max-value. Let's see if they have an effect on merging results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             mean-l1:\tNDCG=0.5489\tF1=0.3095\n",
      "             mean-l2:\tNDCG=0.5544\tF1=0.2991\n",
      "            mean-max:\tNDCG=0.5526\tF1=0.2941\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def normalized_scores(scores, norm):\n",
    "    \"\"\"normalize the given predictions (in sklearn multilabel format)\n",
    "    using the given normalization method ('l1', 'l2' or 'max')\"\"\"\n",
    "\n",
    "for norm in ('l1', 'l2', 'max'):\n",
    "    # we will have to perform the normalization separately for each algorithm's output\n",
    "    Escores_norm = np.zeros(Escores.shape)\n",
    "    for algoid in range(len(ALGORITHMS)):\n",
    "        Escores_norm[:,:,algoid] = normalize(Escores[:,:,algoid], norm=norm)\n",
    "    Epreds_norm = multilabel_to_ranking_preds(Escores_norm, Eqids, Ecids)\n",
    "    eval_results[\"mean-\" + norm] = evaluate(\"mean-\" + norm, Eqids, Ey, Etrue, Epreds_norm.mean(1), Ecids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not seem to help much either. All normalization methods give similar results as the mean method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: PAV aka Isotonic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning to Rank\n",
    "\n",
    "Let's try a true machine learning based fusion approach. We will apply the state of the art Learning to Rank algorithm LambdaMART, which is typically used for ranking results in a search engine. In this case, we consider each document a \"query\" and the predicted subjects as \"results\" and will try to come up with the ideal ranking of those subjects so that gold standard subjects are ranked as high as possible while non-relevant subjects are ranked lower. The algorithm is given a ranking measure that it then attempts to optimize; we will use NDCG as the measure to optimize.\n",
    "\n",
    "\n",
    "\n",
    "The LambdaMART algorithm is conveniently implemented in the `pyltr` library. This implementation has some advanced features: it can be connected to a monitor that periodically evaluates the learned model on validation data and keeps track of the learning progress. If the monitor notices that the learning has reached a plateau - no improvement in validation scores has been made in the last K training rounds - it can stop the learning as well as roll back the model to the state when it reached that plateau. This is called *early stopping* and *trimming*, respectively, and it should help guard against overfitting the model on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "\n",
    "Any Learning to Rank algorithm requires features for the \"results\" (i.e. subjects in our case); here we will simply use the raw scores predicted by the subject indexing algorithms as the features. In addition, we will use the lengths of the original input documents as features, as well as some concept features such as label length. These are all quite easy for us to generate, but we don't know whether the LTR algorithm has any use for them - let's give it whatever features we can think of and let it make its own decisions on whether to use them and how.\n",
    "\n",
    "First the document-level features (currently just size):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read information about document lengths from a data file generated using the `du -b` command\n",
    "doc_length = {}\n",
    "with open(FILE_SIZES) as file_sizes:\n",
    "    for line in file_sizes:\n",
    "        size, filename = line.strip().split()\n",
    "        basename = filename[:-4].split('-')[-1]\n",
    "        doc_length[int(basename)] = int(size)\n",
    "\n",
    "# define a function that looks up document lengths based on qids and returns a 1D feature matrix\n",
    "def doc_features(qids):\n",
    "    \"\"\"return a NumPy array of document features (in practice, file sizes) corresponding to the given qids\"\"\"\n",
    "    \n",
    "    return np.array([np.array([doc_length[qid]]) for qid in qids])\n",
    "\n",
    "# let's also make note of what document features we used, for later display\n",
    "DOCUMENT_FEATURES = ('d-length',)\n",
    "\n",
    "# generate document features for train, validate and test sets\n",
    "Tdocf = doc_features(Tqids)\n",
    "Vdocf = doc_features(Vqids)\n",
    "Edocf = doc_features(Eqids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we generate some concept level features. We will generate several features per concept, based mainly on concept labels: label length (characters and words), number and proportion of capital letters etc. Also we will count the number of occurrences of each concept in the train set and use that as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the per-concept frequencies from the validation(!) set into a 1D NumPy array with size n_concepts\n",
    "\n",
    "# To (try to) avoid overfitting, we can use just the base 10 or natural logarithms of the frequencies (incremented by\n",
    "# one to avoid divide by zero) and round them to the nearest integer to indicate rough magnitude\n",
    "#concept_freq = np.log(Vtrue.sum(axis=0) + 1).round()    # natural logarithm\n",
    "#concept_freq = np.log10(Vtrue.sum(axis=0) + 1).round()  # base 10 logarithm\n",
    "concept_freq = Vtrue.sum(axis=0)                         # raw frequency value\n",
    "\n",
    "def features_of_concept(cid):\n",
    "    \"\"\"return a 1D NumPy array of features for a particular concept\"\"\"\n",
    "    \n",
    "    features = []\n",
    "    features.append(concept_freq[cid])  # frequency in the train set\n",
    "    label = subject_label[cid]\n",
    "    features.append(len(label))  # label length in characters\n",
    "    features.append(len(label.split()))  # label length in words\n",
    "    caps = sum(1 for c in label if c.isupper())\n",
    "    features.append(caps)  # number of capital letters\n",
    "    features.append(caps / len(label))  # proportion of capital letters\n",
    "    return np.array(features)\n",
    "\n",
    "def concept_features(cids):\n",
    "    \"\"\"return a 2D NumPy array of features for the given concepts\"\"\"\n",
    "    \n",
    "    return np.array([features_of_concept(cid) for cid in cids])\n",
    "\n",
    "# let's also make note of what concept features we used, for later display\n",
    "CONCEPT_FEATURES = ('c-freq', 'c-chars', 'c-words', 'c-caps', 'c-capprop')\n",
    "\n",
    "# generate concept features for train, validate and test sets\n",
    "Tconcf = concept_features(Tcids)\n",
    "Vconcf = concept_features(Vcids)\n",
    "Econcf = concept_features(Ecids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can combine the features into matrices for feeding into the LTR algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(283089, 9)\n",
      "(22604, 9)\n",
      "(33318, 9)\n"
     ]
    }
   ],
   "source": [
    "TXall = np.hstack((TX, Tdocf, Tconcf))\n",
    "print(TXall.shape)\n",
    "\n",
    "VXall = np.hstack((VX, Vdocf, Vconcf))\n",
    "print(VXall.shape)\n",
    "\n",
    "EXall = np.hstack((EX, Edocf, Econcf))\n",
    "print(EXall.shape)\n",
    "\n",
    "# make note what features we used\n",
    "ALL_FEATURES = ALGORITHMS + DOCUMENT_FEATURES + CONCEPT_FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can run the LTR algorithm itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter  Train score  OOB Improve    Remaining                           Monitor Output \n",
      "    1       0.5718       0.5618       70.48m      C:      0.5217 B:      0.5217 S:  0\n",
      "    2       0.6155       0.0594       73.20m      C:      0.5733 B:      0.5733 S:  0\n",
      "    3       0.6318       0.0148       74.47m      C:      0.5929 B:      0.5929 S:  0\n",
      "    4       0.6455       0.0031       74.60m      C:      0.5969 B:      0.5969 S:  0\n",
      "    5       0.6525       0.0042       74.89m      C:      0.6067 B:      0.6067 S:  0\n",
      "    6       0.6740       0.0062       75.13m      C:      0.6100 B:      0.6100 S:  0\n",
      "    7       0.6598       0.0124       75.23m      C:      0.6232 B:      0.6232 S:  0\n",
      "    8       0.6636       0.0025       75.32m      C:      0.6281 B:      0.6281 S:  0\n",
      "    9       0.6633       0.0017       75.45m      C:      0.6365 B:      0.6365 S:  0\n",
      "   10       0.6820       0.0109       75.54m      C:      0.6432 B:      0.6432 S:  0\n",
      "   15       0.7031       0.0006       75.64m      C:      0.6844 B:      0.6844 S:  0\n",
      "   20       0.7090       0.0028       75.58m      C:      0.6976 B:      0.6976 S:  0\n",
      "   25       0.7081       0.0010       75.72m      C:      0.7053 B:      0.7053 S:  0\n",
      "   30       0.7365       0.0003       75.43m      C:      0.7077 B:      0.7089 S:  1\n",
      "   35       0.7120       0.0002       75.10m      C:      0.7066 B:      0.7089 S:  6\n",
      "   40       0.6974      -0.0006       74.66m      C:      0.7055 B:      0.7103 S:  4\n",
      "   45       0.7324       0.0005       74.41m      C:      0.7100 B:      0.7103 S:  9\n",
      "   50       0.7214      -0.0003       74.12m      C:      0.7062 B:      0.7103 S: 14\n",
      "   60       0.6980      -0.0006       73.47m      C:      0.7021 B:      0.7103 S: 24\n",
      "   70       0.7183      -0.0005       72.69m      C:      0.7044 B:      0.7103 S: 34\n",
      "   80       0.6978      -0.0004       71.98m      C:      0.7059 B:      0.7103 S: 44\n",
      "Early termination at iteration  85\n",
      "CPU times: user 6min 43s, sys: 68 ms, total: 6min 43s\n",
      "Wall time: 6min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create a monitor for early stopping and trimming, using validation data\n",
    "monitor = pyltr.models.monitors.ValidationMonitor(\n",
    "    VXall, Vy, Vqids, metric=ndcg_metric, stop_after=50)\n",
    "\n",
    "# create a LambdaMART model for learning a suitable ranking\n",
    "model = pyltr.models.LambdaMART(\n",
    "    metric=ndcg_metric,\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.2,\n",
    "    query_subsample=0.1,\n",
    "    max_leaf_nodes=10,\n",
    "    min_samples_leaf=64,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# fit the model to training data\n",
    "model.fit(TXall, Ty, Tqids, monitor=monitor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a model trained on the training documents and validated on the validation documents. First, since we spent so much time creating the model, let's save it for later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ltr-model.joblib']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the learned model for later use\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(model, 'ltr-model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the model to predict subject rankings on the evaluation documents and evaluate how well it did compared to the other fusion approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 ltr:\tNDCG=0.5952\tF1=0.3199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oisuomin/.local/share/virtualenvs/annif-fusion-zWzWviAB/lib/python3.5/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "Epred = model.predict(EX)\n",
    "eval_results[\"ltr\"] = evaluate(\"ltr\", Eqids, Ey, Etrue, Epred, Ecids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LTR results for test set\n",
    "\n",
    "### old, bad Maui scores; using only raw scores as features (3 features)\n",
    "* max_pred=25, n_estimators=1000, query_subsample=0.5, max_leaf_nodes=10, min_samples_leaf=64: \n",
    "  * after 52(+50) iterations: NDCG=0.6097\tF1=0.3108\n",
    "* max_pred=50, n_estimators=1000, query_subsample=0.5, max_leaf_nodes=10, min_samples_leaf=64: \n",
    "  * after 98(+30) iterations: NDCG=0.5745\tF1=0.3056\n",
    "* max_pred=50, n_estimators=1000, query_subsample=0.25, max_leaf_nodes=10, min_samples_leaf=64: \n",
    "  * after 90(+30) iterations: NDCG=0.5828\tF1=0.3135\n",
    "* max_pred=50, n_estimators=1000, query_subsample=0.1, max_leaf_nodes=10, min_samples_leaf=64: \n",
    "  * after 54(+30) iterations: NDCG=0.5825\tF1=0.3053 (6min 15s)\n",
    "\n",
    "### old, bad Maui scores; using raw score + doc length + concept label length&caps (=8 features)\n",
    "\n",
    "* max_pred=50, n_estimators=1000, query_subsample=0.1, max_leaf_nodes=10, min_samples_leaf=64: \n",
    "  * after 78(+30) iterations: NDCG=0.5848\tF1=0.3076 (8min 28s)\n",
    "* max_pred=50, n_estimators=1000, learning_rate=0.2, query_subsample=0.1, max_leaf_nodes=10, min_samples_leaf=64: \n",
    "  * after 66(+50) iterations: NDCG=0.5715\tF1=0.3039 (8min 54s)\n",
    "\n",
    "### new Maui scores; using 8 features\n",
    "\n",
    "* max_pred=50, n_estimators=1000, learning_rate=0.2, query_subsample=0.1, max_leaf_nodes=10, min_samples_leaf=64: \n",
    "  * after 40(+50) iterations: NDCG=0.5925\tF1=0.3159 (7min 13s)\n",
    "  \n",
    "### new Maui scores: using 9 features (all above + train concept freq)\n",
    "\n",
    "* max_pred=50, n_estimators=1000, learning_rate=0.2, query_subsample=0.1, max_leaf_nodes=10, min_samples_leaf=64: \n",
    "  * after 135(+50) iterations: NDCG=0.3777\tF1=0.0069 (15min 37s) ??? **what went wrong?** Apparently the model got too hooked on c-freq (importance 0.2477 vs. Maui 0.3144) and this didn't generalize so well...\n",
    "  \n",
    "### new Maui scores: using 9 features (8 basic + log10 of train concept freq)\n",
    "\n",
    "* max_pred=50, n_estimators=1000, learning_rate=0.2, query_subsample=0.1, max_leaf_nodes=10, min_samples_leaf=64:\n",
    "  * after 58(+50) iterations: NDCG=0.5722\tF1=0.2821 (9min 21s) -- **better than raw freq but still not good**\n",
    "\n",
    "### new Maui scores: using 9 features (8 basic + logN of train concept freq)\n",
    "* max_pred=50, n_estimators=1000, learning_rate=0.2, query_subsample=0.1, max_leaf_nodes=10, min_samples_leaf=64:\n",
    "  * after 69(+50) iterations: NDCG=0.3641\tF1=0.0000 (9min 11s) -- **horrible**\n",
    "  \n",
    "### new Maui scores: using 9 features (8 basic + logN of validate concept freq)\n",
    "* max_pred=50, n_estimators=1000, learning_rate=0.2, query_subsample=0.1, max_leaf_nodes=10, min_samples_leaf=64:\n",
    "  * after 57(+50) iterations: NDCG=0.5973\tF1=0.3234 (8min 52s) -- **new record**\n",
    "  \n",
    "### new Maui scores: using 9 features (8 basic + log10 of validate concept freq)\n",
    "* max_pred=50, n_estimators=1000, learning_rate=0.2, query_subsample=0.1, max_leaf_nodes=10, min_samples_leaf=64:\n",
    "  * after 72(+50) iterations: NDCG=0.5984\tF1=0.3147 (9min 46s)\n",
    "\n",
    "### new Maui scores: using 9 features (8 basic + validate concept freq)\n",
    "* max_pred=50, n_estimators=1000, learning_rate=0.2, query_subsample=0.1, max_leaf_nodes=10, min_samples_leaf=64:\n",
    "  * after 35(+50) iterations: NDCG=0.5952\tF1=0.3199 (6min 44s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check how much the input from each algorithm contributed to the rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               tfidf importance: 0.1672\n",
      "            fasttext importance: 0.1224\n",
      "                maui importance: 0.3144\n",
      "            d-length importance: 0.0771\n",
      "              c-freq importance: 0.2477\n",
      "             c-chars importance: 0.0619\n",
      "             c-words importance: 0.0094\n",
      "              c-caps importance: 0.0000\n",
      "           c-capprop importance: 0.0000\n"
     ]
    }
   ],
   "source": [
    "for feature, importance in zip(ALL_FEATURES, model.feature_importances_):\n",
    "    print(\"{:>20} importance: {:.4f}\".format(feature, importance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: plot the results using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

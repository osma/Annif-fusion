{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annif Fusion experiment\n",
    "\n",
    "**Osma Suominen, October 2018**\n",
    "\n",
    "This notebook is an experiment for evaluating different methods to combine results of multiple subject indexing algorithms. We will use a document collection where gold standard subjects have been manually assigned and compare the subjects assigned by three different algorithm first individually, then in combinations created by different fusion approaches. In particular, we will test simple mean-of-subject-scores and union-of-top-K approaches as well as more advanced methods such as score normalization, isotonic regression and Learning to Rank style machine learning. Ideally, we would like to find a method for combining results from multiple algorithm that gives us the best quality results, combining the strengths of individual algorithms (both statistical/associative and lexical) while eliminating the effects of their weaknesses. The findings will then inform the further development of Annif.\n",
    "\n",
    "The experiment was inspired by the Martin Toepfer's paper [Fusion architectures for automatic subject indexing under concept drift](https://link.springer.com/article/10.1007/s00799-018-0240-3) as well as discussions with him during and after the NKOS2018 workshop.\n",
    "\n",
    "If you want to run this yourself, you need Python 3.5+ with the following libraries:\n",
    "\n",
    "* jupyter (obviously)\n",
    "* scikit-learn\n",
    "* pyltr\n",
    "* matplotlib (for the final plots)\n",
    "\n",
    "This document is (c) Osma Suominen. It may be shared and reused according to the terms of the [CC Attribution 4.0 International](https://creativecommons.org/licenses/by/4.0/) license. Attribution should include the name of the author and a link to the original source document. However, code snippets in this notebook may be freely reused according to the [CC0 1.0 Universal](https://creativecommons.org/publicdomain/zero/1.0/) license. Attribution for code is requested but not required.\n",
    "\n",
    "## Document corpus\n",
    "\n",
    "We will use the [\"Ask a Librarian\" document corpus](https://github.com/NatLibFi/Annif-corpora/tree/master/fulltext/kirjastonhoitaja) for the experiment. It contains 3150 short question-answer pairs in Finnish language. The collection is subdivided into three subsets: train (n=2625), validate (n=213) and test (n=312). All the final evaluations will be performed on the test set, which contains the most recent questions asked during the year 2017. However, for some of the fusion methods we will make use of the train and validate subsets in order to fine-tune the way results are combined. The documents are represented as `*.txt` files.\n",
    "\n",
    "All documents in the collection have been assigned gold standard subjects by librarians, stored in `*.tsv` files where the basenames correspond to the `*.txt` files. Librarians have assigned at least 4 subjects per document (average is 4.4-4.8 subjects per document, depending on the subset).\n",
    "\n",
    "In addition, the documents have been analyzed using the [Annif](https://github.com/NatLibFi/Annif/) tool (development version v0.31.0) using three independent automated subject indexing algorithms: TF-IDF vector similarity, fastText and Maui, asking each algorithm to suggest up to 1000 subjects per document (however, we will in practice only load a fraction of these) with scores ranging from 1.0 to 0.0 given to each suggested subject. The subjects suggested by these algoritms are stored, respectively, in `*.tfidf`, `*.fasttext` and `*.maui` files.\n",
    "\n",
    "First we load some basic modules and define the locations of the data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyltr\n",
    "import os\n",
    "import os.path\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "SUBJECTFILE='data/yso.tsv'\n",
    "TRAIN_DIR='data/kirjastonhoitaja/train/'\n",
    "VALI_DIR='data/kirjastonhoitaja/validate/'\n",
    "TEST_DIR='data/kirjastonhoitaja/test/'\n",
    "ALGORITHMS = ('tfidf','fasttext','maui')  # these correspond to file extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject vocabulary\n",
    "\n",
    "All the subjects have been chosen from the Finnish General Ontology YSO, which contains around 28000 concepts.\n",
    "Here we load the vocabulary from a TSV file where the first column is the concept URI and the second column is the concept label in Finnish. This has been extracted from the full YSO SKOS file.\n",
    "\n",
    "Hereafter we will only use integer concept IDs which range from 0 to (n_concepts-1). All calculations are performed using the concept IDs so we just need to map the concept URIs which appear in files to their IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary loaded with 27760 concepts\n"
     ]
    }
   ],
   "source": [
    "uri_to_cid = {}\n",
    "with open(SUBJECTFILE) as subjf:\n",
    "    for cid, line in enumerate(subjf):\n",
    "        uri = line.strip().split(\"\\t\")[0]\n",
    "        uri_to_cid[uri] = cid\n",
    "n_concepts = len(uri_to_cid)\n",
    "print(\"Vocabulary loaded with\", n_concepts, \"concepts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document corpus\n",
    "\n",
    "We define a function to load the data from a directory of files in the format explained above. To be able to use both `scikit-learn` and `pyltr` libraries, we will need to express the data in two somewhat different formats, both based on multidimensional NumPy arrays:\n",
    "    \n",
    "1. pyltr ranking format, where the NumPy array rows correspond to document-subject pairs, i.e. there are multiple rows per document, one row per subject that has been suggested by at least one algorithm. A separate qids array (\"query IDs\", actually document IDs when applied this way) maps the document-subject pairs to the individual documents.\n",
    "2. scikit-learn multilabel format, where the NumPy array has one row per document and the columns are individual concepts (either True/False for the gold standard subjects, or subject scores for the predicted subjects)\n",
    "\n",
    "We will define this function once and then use it to parse all three document subsets (train, validate, test). We will only load up to 25 suggested subjects per document to keep the size of the rankings manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TX shape: (155260, 3)\n",
      "Ty shape: (155260,)\n",
      "Tqids shape: (155260,)\n",
      "Tcids shape: (155260,)\n",
      "Ttrue shape: (2625, 27760)\n",
      "Tscores shape: (2625, 27760, 3)\n",
      "\n",
      "VX shape: (12527, 3)\n",
      "Vy shape: (12527,)\n",
      "Vqids shape: (12527,)\n",
      "Vcids shape: (12527,)\n",
      "Vtrue shape: (213, 27760)\n",
      "Vscores shape: (213, 27760, 3)\n",
      "\n",
      "EX shape: (18292, 3)\n",
      "Ey shape: (18292,)\n",
      "Eqids shape: (18292,)\n",
      "Ecids shape: (18292,)\n",
      "Etrue shape: (312, 27760)\n",
      "Escores shape: (312, 27760, 3)\n"
     ]
    }
   ],
   "source": [
    "def load_data(directory, max_pred=25):\n",
    "    \"\"\"Reads document corpus with gold standard and predicted subjects for each document.\n",
    "    Only the top max_pred predictions per document and algorithm are considered for ranking.\n",
    "    Returns a tuple with\n",
    "     - predictions of individual algorithms as a 2D NumPy array with the shape (doc-suggestions, n_algos)\n",
    "     - relevant (gold standard) concepts as a 1D NumPy array with the shape (doc-suggestions)\n",
    "     - qids (actually document ids) as a 1D NumPy array with length doc-suggestions\n",
    "     - concept IDs of suggested concepts as a 1D NumPy array with shape (doc-suggestions)\n",
    "     - gold standard subjects in sklearn multilabel format as a 2D NumPy array with shape (n_docs, n_concepts) \n",
    "     - predictions of individual algorithms in sklearn multilabel format as a 3D NumPy array with shape (n_docs, n_concepts, n_algos)\n",
    "    \"\"\"\n",
    "    \n",
    "    # temporary lists to be converted into NumPy arrays at the end\n",
    "    docdatalist = []\n",
    "    docscorelist = []\n",
    "    docgoldlist = []\n",
    "    doctruelist = []\n",
    "    qidlist = []\n",
    "    cidlist = []\n",
    "    \n",
    "    for fileid, txtfile in enumerate(glob.glob(os.path.join(directory, '*.txt'))):\n",
    "        basename  = txtfile[:-4]\n",
    "        \n",
    "        # Initialize binary mask containing the top max_pred concepts that were suggested by any algorithm.\n",
    "        # This will be used to pare down the doc-suggestions for ranking so that we will skip concepts\n",
    "        # that were not suggested by any algorithm. They will still be considered in F1 score evaluation,\n",
    "        # which is not based only on the ranked suggestions but used the full gold standard subjects.\n",
    "        mask = np.zeros(n_concepts, dtype=np.bool)\n",
    "\n",
    "        docdata = np.zeros((n_concepts, len(ALGORITHMS)))\n",
    "        for algoid, fileext in enumerate(ALGORITHMS):\n",
    "            with open(basename + \".\" + fileext) as algooutput:\n",
    "                lines = 0\n",
    "                for lineno, line in enumerate(algooutput):\n",
    "                    uri, _, score = line.strip().split(\"\\t\")\n",
    "                    if uri not in uri_to_cid:\n",
    "                        continue  # ignore unknown URIs\n",
    "                    cid = uri_to_cid[uri]\n",
    "                    docdata[cid,algoid] = float(score)\n",
    "                    if lineno < max_pred:\n",
    "                        mask[cid] = True\n",
    "        docdatalist.append(docdata[mask])\n",
    "        docscorelist.append(docdata)\n",
    "    \n",
    "        docgold = np.zeros(n_concepts, dtype=np.bool)\n",
    "        with open(basename + \".tsv\") as tsvfile:\n",
    "            for line in tsvfile:\n",
    "                uri = line.split(\"\\t\")[0]\n",
    "                if uri in uri_to_cid:\n",
    "                    cid = uri_to_cid[uri]\n",
    "                    docgold[cid] = True\n",
    "        docgoldlist.append(docgold[mask])\n",
    "        doctruelist.append(docgold)\n",
    "        \n",
    "        qidlist.append(np.full(mask.sum(), fileid))\n",
    "        cidlist.append(np.arange(n_concepts)[mask])\n",
    "\n",
    "    return np.concatenate(docdatalist), np.concatenate(docgoldlist), np.concatenate(qidlist), \\\n",
    "           np.concatenate(cidlist), np.array(doctruelist), np.array(docscorelist)\n",
    "\n",
    "# now load the train, validate and test documents\n",
    "TX, Ty, Tqids, Tcids, Ttrue, Tscores = load_data(TRAIN_DIR)\n",
    "VX, Vy, Vqids, Vcids, Vtrue, Vscores = load_data(VALI_DIR)\n",
    "EX, Ey, Eqids, Ecids, Etrue, Escores = load_data(TEST_DIR)\n",
    "\n",
    "# Show some information about the shapes\n",
    "print(\"TX shape:\", TX.shape)\n",
    "print(\"Ty shape:\", Ty.shape)\n",
    "print(\"Tqids shape:\", Tqids.shape)\n",
    "print(\"Tcids shape:\", Tcids.shape)\n",
    "print(\"Ttrue shape:\", Ttrue.shape)\n",
    "print(\"Tscores shape:\", Tscores.shape)\n",
    "print()\n",
    "print(\"VX shape:\", VX.shape)\n",
    "print(\"Vy shape:\", Vy.shape)\n",
    "print(\"Vqids shape:\", Vqids.shape)\n",
    "print(\"Vcids shape:\", Vcids.shape)\n",
    "print(\"Vtrue shape:\", Vtrue.shape)\n",
    "print(\"Vscores shape:\", Vscores.shape)\n",
    "print()\n",
    "print(\"EX shape:\", EX.shape)\n",
    "print(\"Ey shape:\", Ey.shape)\n",
    "print(\"Eqids shape:\", Eqids.shape)\n",
    "print(\"Ecids shape:\", Ecids.shape)\n",
    "print(\"Etrue shape:\", Etrue.shape)\n",
    "print(\"Escores shape:\", Escores.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics\n",
    "\n",
    "For evaluating the quality of results, we will use two metrics: [NDCG](https://en.wikipedia.org/wiki/Discounted_cumulative_gain#Normalized_DCG) which is based on ranked suggestions (if the top ranked suggestion is correct it will contribute more to the score than getting the 2nd or 3rd suggestion right) and [F1 score](https://en.wikipedia.org/wiki/F1_score) that only considers binary choices: either a concept is a subject of a document or it is not, there are no fuzzy options. \n",
    "\n",
    "For the **NDCG score**, we can simply use the implementation from the `pyltr` library. Here we limit the NDCG evaluation to the top 20 most highly ranked subjects (i.e. NDCG@20), meaning that subjects ranked 21st, 22nd etc won't contribute to the score (their effect would be quite small anyway do to the discounting performed in NDCG calculations).\n",
    "\n",
    "For the **F1 score**, we can use the `scikit-learn` implementation. However, its usage is complicated by the fact that we are mostly dealing with ranked suggestions in the pyltr format, so we need to convert those into the multilabel prediction format understood by scikit-learn and further binarize the predictions using a thresholding strategy. For simplicity we will pick the top 5 suggested concepts, so we get an amount of subjects that is similar to the librarian-assigned ones (around 4.5 subjects per document), and compare those to the gold standard. The score for each document will be calculated individually and the final result will be the average of those scores (i.e. sample based average).\n",
    "\n",
    "Finally, we will define an evaluation function that is given ranked suggestions as well as gold standard subjects (in both pyltr and scikit-learn formats) and some auxiliary information (qids and cids). It will calculate both NDCG and F1 score, print them out, and also return the scores so that they can be stored for later plotting etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# initialize the NDCG metric implementation\n",
    "ndcg_metric = pyltr.metrics.NDCG(k=20)\n",
    "\n",
    "def ranking_to_multilabel(pred, qids, cids):\n",
    "    \"\"\"convert from pyltr ranking prediction format to sklearn multilabel prediction format\"\"\"\n",
    "    doc_labels = collections.OrderedDict()  # key: qid, val: NumPy 1D array with length n_concepts\n",
    "    for score, qid, cid in zip(pred, qids, cids):\n",
    "        if qid not in doc_labels:\n",
    "            doc_labels[qid] = np.zeros(n_concepts)\n",
    "        doc_labels[qid][cid] = score\n",
    "    return np.array(list(doc_labels.values()))\n",
    "\n",
    "def binarize_multilabel_pred(pred, k=5):\n",
    "    \"\"\"convert predictions with scores to a boolean matrix, taking the top K predictions per document\"\"\"\n",
    "    # I'm pretty sure this could be done easier, perhaps in a single NumPy operation, but I can't figure it out\n",
    "    # Using a loop instead, it's not going to take long anyway\n",
    "    binary_labels = np.zeros(pred.shape, dtype=np.bool)\n",
    "    pred_order = pred.argsort()[:,::-1]\n",
    "    for i, mask in enumerate(pred_order[:,:k]):\n",
    "        binary_labels[i,mask] = True\n",
    "    return binary_labels\n",
    "\n",
    "def evaluate(name, qids, y, y_true, pred, cids):\n",
    "    \"\"\"evaluate a ranking, calculating its NDCG and F1 score. Print and return the scores\"\"\"\n",
    "    # calculate NDCG score\n",
    "    ndcg = ndcg_metric.calc_mean(qids, y, pred)\n",
    "\n",
    "    # calculate F1 score\n",
    "    pred_labels = ranking_to_multilabel(pred, qids, cids)\n",
    "    f1 = f1_score(y_true, binarize_multilabel_pred(pred_labels), average='samples')\n",
    "    \n",
    "    print(\"{:>20}:\\tNDCG={:.4f}\\tF1={:.4f}\".format(name, ndcg, f1))\n",
    "    return ndcg, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline evaluation\n",
    "\n",
    "Now we are ready for some baseline evaluations: we can evaluate the individual algorithms as well as a simplistic combination method using the mean of assigned scores (the only one implemented in Annif v0.31.0). We will store the results for later too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               tfidf:\tNDCG=0.4428\tF1=0.2220\n",
      "            fasttext:\tNDCG=0.2896\tF1=0.1329\n",
      "                maui:\tNDCG=0.3586\tF1=0.1905\n",
      "                mean:\tNDCG=0.5449\tF1=0.2690\n"
     ]
    }
   ],
   "source": [
    "# initialize dictionary for collecting evaluation results\n",
    "eval_results = collections.OrderedDict()\n",
    "\n",
    "# evaluate the individual algorithms\n",
    "for algoid, name in enumerate(ALGORITHMS):\n",
    "    eval_results[name] = evaluate(name, Eqids, Ey, Etrue, EX[:,algoid], Ecids)\n",
    "\n",
    "# evaluate the combination of algorithms using mean of scores\n",
    "eval_results[\"mean\"] = evaluate(\"mean\", Eqids, Ey, Etrue, EX.mean(1), Ecids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that out of individual algorithms, TF-IDF performed best, followed by Maui and fastText. The mean of scores gave better results than any algorithm alone. The NDCG and F1 measures seem to agree on which methods are better than others, as they both ranked the methods in the same order.\n",
    "\n",
    "Note that the F1 scores may seem quite low (in some classification tasks F1 scores of 0.95 and more are not uncommon), but keep in mind that the task given to the algorithms is quite difficult: for each document, it should pick the 5 subjects (out of nearly 28000) that best describe that document. There are numerous ways of getting this wrong and only a few ways of picking a good set of subjects. So even a rather low F1 score of 0.2 means that around one out of five subjects was right - much better than chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Union of top K concepts\n",
    "\n",
    "Let's try a variation of averaging the scores of the different algorithms. This time we only pick the top K subjects (where K is a small number, e.g. 2 or 3) suggested by each algorithm and use their union as the suggested concepts. We will still take the mean of scores in order to rank the results (otherwise the NDCG score would not be well defined as it requires the results to have some stable ranking order) but will drastically reduce the number of subjects being considered.\n",
    "\n",
    "This is a bit challenging to implement within the ranked suggestions format of pyltr, so what we will do instead is to implement it using sklearn multilabel prediction format and then convert that to a list of ranking predictions using the qids and cids information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             union-1:\tNDCG=0.2508\tF1=0.1370\n",
      "             union-2:\tNDCG=0.3367\tF1=0.2149\n",
      "             union-3:\tNDCG=0.3870\tF1=0.2259\n",
      "             union-5:\tNDCG=0.4503\tF1=0.2248\n",
      "            union-10:\tNDCG=0.5171\tF1=0.2439\n",
      "            union-20:\tNDCG=0.5112\tF1=0.2632\n",
      "            union-40:\tNDCG=0.5296\tF1=0.2696\n"
     ]
    }
   ],
   "source": [
    "def multilabel_to_ranking_preds(scores, qids, cids):\n",
    "    \"\"\"convert a sklearn multilabel prediction to pyltr ranking according to the given qids and cids\"\"\"\n",
    "    \n",
    "    return np.array([scores[qid,cid] for qid, cid in zip(qids, cids)])\n",
    "\n",
    "def select_top_k(scores, k):\n",
    "    mask = binarize_multilabel_pred(scores, k=k)\n",
    "    return scores * mask\n",
    "\n",
    "def union_of_top_k(scores, k):\n",
    "    top_scores = np.zeros(scores.shape)\n",
    "    for algoid in range(len(ALGORITHMS)):\n",
    "        top_scores[:,:,algoid] = select_top_k(scores[:,:,algoid], k=k)\n",
    "    \n",
    "    preds_union = multilabel_to_ranking_preds(top_scores, Eqids, Ecids)\n",
    "    name = \"union-{}\".format(k)\n",
    "    eval_results[name] = evaluate(name, Eqids, Ey, Etrue, preds_union.mean(1), Ecids)\n",
    "\n",
    "for k in (1,2,3,5,10,20,40):\n",
    "    union_of_top_k(Escores, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results were not encouraging. NDCG and F1 scores are generally lower than for the mean strategy. With higher K values, the results get closer to the ones for the mean method, which is expected since having a low limit on the number of results considered is the only aspect that differentiates this method from using just the mean of all scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization of prediction scores\n",
    "\n",
    "Each algorithm returns its scores on the scale 0.0 to 0.1, but they may be using different ranges within that scale. Because of this, simple averaging of scores may cause one algorithm which tends to report high scores to have much more impact on the result than another one that uses low scores. We can counteract that by normalizing the scores before taking the mean of scores. `scikit-learn` implements several normalization methods: L1, L2 and max-value. Let's see if they have an effect on merging results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             mean-l1:\tNDCG=0.5043\tF1=0.2601\n",
      "             mean-l2:\tNDCG=0.5381\tF1=0.2671\n",
      "            mean-max:\tNDCG=0.5592\tF1=0.2763\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def normalized_scores(scores, norm):\n",
    "    \"\"\"normalize the given predictions (in sklearn multilabel format)\n",
    "    using the given normalization method ('l1', 'l2' or 'max')\"\"\"\n",
    "\n",
    "for norm in ('l1', 'l2', 'max'):\n",
    "    # we will have to perform the normalization separately for each algorithm's output\n",
    "    Escores_norm = np.zeros(Escores.shape)\n",
    "    for algoid in range(len(ALGORITHMS)):\n",
    "        Escores_norm[:,:,algoid] = normalize(Escores[:,:,algoid], norm=norm)\n",
    "    Epreds_norm = multilabel_to_ranking_preds(Escores_norm, Eqids, Ecids)\n",
    "    eval_results[\"mean-\" + norm] = evaluate(\"mean-\" + norm, Eqids, Ey, Etrue, Epreds_norm.mean(1), Ecids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not seem to help much either. The `max` normalization strategy is slightly better than the mean strategy but the difference is not significant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: PAV aka Isotonic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning to Rank\n",
    "\n",
    "Let's try a true machine learning based fusion approach. We will apply the state of the art Learning to Rank algorithm LambdaMART, which is typically used for ranking results in a search engine. In this case, we consider each document a \"query\" and the predicted subjects as \"results\" and will try to come up with the ideal ranking of those subjects so that gold standard subjects are ranked as high as possible while non-relevant subjects are ranked lower. The algorithm is given a ranking measure that it then attempts to optimize; we will use NDCG as the measure to optimize.\n",
    "\n",
    "A Learning to Rank algorithm requires features for the \"results\" (i.e. subjects in our case); here we will simply use the raw scores predicted by the subject indexing algorithms as the features.\n",
    "\n",
    "The LambdaMART algorithm is conveniently implemented in the `pyltr` library. This implementation has some advanced features: it can be connected to a monitor that periodically evaluates the learned model on validation data and keeps track of the learning progress. If the monitor notices that the learning has reached a plateau - no improvement in validation scores has been made in the last K training rounds - it can stop the learning as well as roll back the model to the state when it reached that plateau. This is called *early stopping* and *trimming*, respectively, and it should help guard against overfitting the model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter  Train score    Remaining                           Monitor Output \n",
      "    1 11523004882363.7383      307.52m C:88106363428735.7812 B:88106363428735.7812 S:  0\n",
      "    2 12409679394611.7051      328.83m C:88290414576678.9062 B:88290414576678.9062 S:  0\n",
      "    3 12409679394611.7051      339.63m C:88290414576678.9062 B:88290414576678.9062 S:  1\n",
      "    4 13187910282989.6738      342.04m C:88290414576678.9062 B:88290414576678.9062 S:  2\n",
      "    5 13187910282989.6738      343.50m C:88290414576678.9062 B:88290414576678.9062 S:  3\n",
      "    6 13187910282989.6738      343.59m C:88290414576678.9062 B:88290414576678.9062 S:  4\n",
      "    7 13187910282989.6738      343.95m C:88290414576678.9062 B:88290414576678.9062 S:  5\n",
      "    8 13231512928182.7344      344.42m C:88290414576678.9062 B:88290414576678.9062 S:  6\n",
      "    9 13240851747143.0078      344.99m C:88290414576678.9062 B:88290414576678.9062 S:  7\n",
      "   10 13240851747143.0078      344.77m C:88290414576678.9062 B:88290414576678.9062 S:  8\n",
      "   15 13761587057452.0293      344.57m C:90348606645535.1562 B:90348606645535.1562 S:  0\n",
      "   20 14136648712436.2852      343.46m C:98152102534216.4688 B:98152102534216.4688 S:  4\n",
      "   25 14630047004671.6094      342.22m C:82442816131292.6094 B:98152102534216.4688 S:  9\n",
      "   30 14748982253139.6914      340.67m C:90153204787310.0781 B:98152102534216.4688 S: 14\n",
      "   35 14786876055123.5352      338.86m C:90153204787310.0781 B:98152102534216.4688 S: 19\n",
      "   40 14982542536397.8906      337.11m C:90153204787310.0625 B:98152102534216.4688 S: 24\n",
      "   45 14957777231544.1621      335.36m C:89969153639366.9688 B:98152102534216.4688 S: 29\n",
      "   50 15040857374137.7012      333.53m C:82165657750685.8594 B:98152102534216.4688 S: 34\n",
      "   60 15002963572153.8555      330.13m C:74362161862004.5469 B:98152102534216.4688 S: 44\n",
      "   70 15002963572153.8555      326.81m C:74362161862004.5469 B:98152102534216.4688 S: 54\n",
      "   80 14808841798753.2715      323.19m C:74362161862004.5469 B:98152102534216.4688 S: 64\n",
      "   90 14907248920665.7344      319.71m C:74362161862004.5625 B:98152102534216.4688 S: 74\n",
      "  100 14907248920665.7344      316.31m C:74362161862004.5625 B:98152102534216.4688 S: 84\n",
      "Early termination at iteration  115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyltr.models.lambdamart.LambdaMART at 0x7f6cfe58ba58>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a monitor for early stopping and trimming, using validation data\n",
    "monitor = pyltr.models.monitors.ValidationMonitor(\n",
    "    VX, Vy, Vqids, metric=ndcg_metric, stop_after=100)\n",
    "\n",
    "# create a LambdaMART model for learning a suitable ranking\n",
    "model = pyltr.models.LambdaMART(\n",
    "    metric=ndcg_metric,\n",
    "    n_estimators=1000,\n",
    "    max_leaf_nodes=10,\n",
    "    min_samples_leaf=64,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# fit the model to training data\n",
    "model.fit(TX, Ty, Tqids, monitor=monitor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a model trained on the training documents and validated on the validation documents. First, since we spent so much time creating the model, let's save it for later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ltr-model.joblib']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the learned model for later use\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(model, 'ltr-model.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the model to predict subject rankings on the evaluation documents and evaluate how well it did compared to the other fusion approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 ltr:\tNDCG=0.5347\tF1=0.2938\n"
     ]
    }
   ],
   "source": [
    "Epred = model.predict(EX)\n",
    "eval_results[\"ltr\"] = evaluate(\"ltr\", Eqids, Ey, Etrue, Epred, Ecids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check how much the input from each algorithm contributed to the rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               tfidf importance: 0.1164\n",
      "            fasttext importance: 0.5298\n",
      "                maui importance: 0.3537\n"
     ]
    }
   ],
   "source": [
    "for feature, importance in zip(ALGORITHMS, model.feature_importances_):\n",
    "    print(\"{:>20} importance: {:.4f}\".format(feature, importance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: plot the results using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

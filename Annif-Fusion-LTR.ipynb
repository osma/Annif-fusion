{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annif Fusion experiment\n",
    "\n",
    "**Osma Suominen, October 2018**\n",
    "\n",
    "This notebook is an experiment for evaluating different methods to combine results of multiple subject indexing algorithms. We will use a document collection where gold standard subjects have been manually assigned and compare the subjects assigned by three different algorithm first individually, then in combinations created by different fusion approaches. In particular, we will test simple mean-of-subject-scores and union-of-top-K approaches as well as more advanced methods such as score normalization, isotonic regression and Learning to Rank style machine learning. Ideally, we would like to find a method for combining results from multiple algorithm that gives us the best quality results, combining the strengths of individual algorithms (both statistical/associative and lexical) while eliminating the effects of their weaknesses. The findings will then inform the further development of Annif.\n",
    "\n",
    "The experiment was inspired by the Martin Toepfer's paper [Fusion architectures for automatic subject indexing under concept drift](https://link.springer.com/article/10.1007/s00799-018-0240-3) as well as discussions with him during and after the NKOS2018 workshop.\n",
    "\n",
    "If you want to run this yourself, you need Python 3.5+ with the following libraries:\n",
    "\n",
    "* jupyter (obviously)\n",
    "* scikit-learn\n",
    "* pyltr\n",
    "* matplotlib (for the final plots)\n",
    "\n",
    "This document is (c) Osma Suominen. It may be shared and reused according to the terms of the [CC Attribution 4.0 International](https://creativecommons.org/licenses/by/4.0/) license. Attribution should include the name of the author and a link to the original source document. However, code snippets in this notebook may be freely reused according to the [CC0 1.0 Universal](https://creativecommons.org/publicdomain/zero/1.0/) license. Attribution for code is requested but not required.\n",
    "\n",
    "## Document corpus\n",
    "\n",
    "We will use the [\"Ask a Librarian\" document corpus](https://github.com/NatLibFi/Annif-corpora/tree/master/fulltext/kirjastonhoitaja) for the experiment. It contains 3150 short question-answer pairs in Finnish language. The collection is subdivided into three subsets: train (n=2625), validate (n=213) and test (n=312). All the final evaluations will be performed on the test set, which contains the most recent questions asked during the year 2017. However, for some of the fusion methods we will make use of the train and validate subsets in order to fine-tune the way results are combined. The documents are represented as `*.txt` files.\n",
    "\n",
    "All documents in the collection have been assigned gold standard subjects by librarians, stored in `*.tsv` files where the basenames correspond to the `*.txt` files. Librarians have assigned at least 4 subjects per document (average is 4.4-4.8 subjects per document, depending on the subset).\n",
    "\n",
    "In addition, the documents have been analyzed using the [Annif](https://github.com/NatLibFi/Annif/) tool (development version v0.31.0) using three independent automated subject indexing algorithms: TF-IDF vector similarity, fastText and Maui, asking each algorithm to suggest up to 1000 subjects per document (however, we will in practice only load a fraction of these) with scores ranging from 1.0 to 0.0 given to each suggested subject. The subjects suggested by these algoritms are stored, respectively, in `*.tfidf`, `*.fasttext` and `*.maui` files.\n",
    "\n",
    "First we load some basic modules and define the locations of the data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyltr\n",
    "import os\n",
    "import os.path\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "SUBJECTFILE='data/yso.tsv'\n",
    "TRAIN_DIR='data/kirjastonhoitaja/train/'\n",
    "VALI_DIR='data/kirjastonhoitaja/validate/'\n",
    "TEST_DIR='data/kirjastonhoitaja/test/'\n",
    "ALGORITHMS = ('tfidf','fasttext','maui')  # these correspond to file extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject vocabulary\n",
    "\n",
    "All the subjects have been chosen from the Finnish General Ontology YSO, which contains around 28000 concepts.\n",
    "Here we load the vocabulary from a TSV file where the first column is the concept URI and the second column is the concept label in Finnish. This has been extracted from the full YSO SKOS file.\n",
    "\n",
    "Hereafter we will only use integer concept IDs which range from 0 to (n_concepts-1). All calculations are performed using the concept IDs so we just need to map the concept URIs which appear in files to their IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary loaded with 27760 concepts\n"
     ]
    }
   ],
   "source": [
    "uri_to_cid = {}\n",
    "with open(SUBJECTFILE) as subjf:\n",
    "    for cid, line in enumerate(subjf):\n",
    "        uri = line.strip().split(\"\\t\")[0]\n",
    "        uri_to_cid[uri] = cid\n",
    "n_concepts = len(uri_to_cid)\n",
    "print(\"Vocabulary loaded with\", n_concepts, \"concepts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document corpus\n",
    "\n",
    "We define a function to load the data from a directory of files in the format explained above. To be able to use both `scikit-learn` and `pyltr` libraries, we will need to express the data in two somewhat different formats, both based on multidimensional NumPy arrays:\n",
    "    \n",
    "1. pyltr ranking format, where the NumPy array rows correspond to document-subject pairs, i.e. there are multiple rows per document, one row per subject that has been suggested by at least one algorithm. A separate qids array (\"query IDs\", actually document IDs when applied this way) maps the document-subject pairs to the individual documents.\n",
    "2. scikit-learn multilabel format, where the NumPy array has one row per document and the columns are individual concepts (either True/False for the gold standard subjects, or subject scores for the predicted subjects)\n",
    "\n",
    "We will define this function once and then use it to parse all three document subsets (train, validate, test). We will only load up to 25 suggested subjects per document to keep the size of the rankings manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TX shape: (160879, 3)\n",
      "Ty shape: (160879,)\n",
      "Tqids shape: (160879,)\n",
      "Tcids shape: (160879,)\n",
      "Ttrue shape: (2625, 27760)\n",
      "\n",
      "VX shape: (12976, 3)\n",
      "Vy shape: (12976,)\n",
      "Vqids shape: (12976,)\n",
      "Vcids shape: (12976,)\n",
      "Vtrue shape: (213, 27760)\n",
      "\n",
      "EX shape: (18951, 3)\n",
      "Ey shape: (18951,)\n",
      "Eqids shape: (18951,)\n",
      "Ecids shape: (18951,)\n",
      "Etrue shape: (312, 27760)\n"
     ]
    }
   ],
   "source": [
    "def load_data(directory, max_pred=25):\n",
    "    \"\"\"Reads document corpus with gold standard and predicted subjects for each document.\n",
    "    Only the top max_pred predictions per document and algorithm are considered.\n",
    "    Returns a tuple with\n",
    "     - predictions of individual algorithms as a 2D NumPy array with the shape (doc-suggestions, n_algos)\n",
    "     - relevant (gold standard) concepts as a 1D NumPy array with the shape (doc-suggestions)\n",
    "     - qids (actually document ids) as a 1D NumPy array with length doc-suggestions\n",
    "     - gold standard subjects in sklearn multilabel format as a 2D NumPy array with shape (n_docs, n_concepts) \n",
    "     - concept IDs of suggested concepts as a 1D NumPy array with shape (doc-suggestions)\n",
    "    \"\"\"\n",
    "    \n",
    "    # temporary lists to be converted into NumPy arrays at the end\n",
    "    docdatalist = []  \n",
    "    docgoldlist = []\n",
    "    doctruelist = []\n",
    "    qidlist = []\n",
    "    cidlist = []\n",
    "    \n",
    "    for fileid, txtfile in enumerate(glob.glob(os.path.join(directory, '*.txt'))):\n",
    "        basename  = txtfile[:-4]\n",
    "        \n",
    "        # Initialize binary mask containing all the concepts that were suggested by any algorithm\n",
    "        # This will be used to pare down the doc-suggestions for ranking so that we will skip concepts\n",
    "        # that were not suggested by any algorithm. They will still be considered in F1 score evaluation,\n",
    "        # which is not based only on the ranked suggestions but used the full gold standard subjects.\n",
    "        mask = np.zeros(n_concepts, dtype=np.bool)\n",
    "\n",
    "        docdata = np.zeros((n_concepts, len(ALGORITHMS)))\n",
    "        for algoid, fileext in enumerate(ALGORITHMS):\n",
    "            with open(basename + \".\" + fileext) as algooutput:\n",
    "                lines = 0\n",
    "                for line in algooutput:\n",
    "                    uri, _, score = line.strip().split(\"\\t\")\n",
    "                    if uri in uri_to_cid:\n",
    "                        cid = uri_to_cid[uri]\n",
    "                        docdata[cid,algoid] = float(score)\n",
    "                        mask[cid] = True\n",
    "                    lines += 1\n",
    "                    if lines > max_pred:\n",
    "                        break # read only top max_pred suggestions per algorithm\n",
    "        docdatalist.append(docdata[mask])\n",
    "    \n",
    "        docgold = np.zeros(n_concepts, dtype=np.bool)\n",
    "        with open(basename + \".tsv\") as tsvfile:\n",
    "            for line in tsvfile:\n",
    "                uri = line.split(\"\\t\")[0]\n",
    "                if uri in uri_to_cid:\n",
    "                    cid = uri_to_cid[uri]\n",
    "                    docgold[cid] = True\n",
    "        docgoldlist.append(docgold[mask])\n",
    "        doctruelist.append(docgold)\n",
    "        \n",
    "        qidlist.append(np.full(mask.sum(), fileid))\n",
    "        cidlist.append(np.arange(n_concepts)[mask])\n",
    "\n",
    "    return np.concatenate(docdatalist), np.concatenate(docgoldlist), np.concatenate(qidlist), \\\n",
    "           np.concatenate(cidlist), np.array(doctruelist)\n",
    "\n",
    "# now load the train, validate and test documents\n",
    "TX, Ty, Tqids, Tcids, Ttrue = load_data(TRAIN_DIR)\n",
    "VX, Vy, Vqids, Vcids, Vtrue = load_data(VALI_DIR)\n",
    "EX, Ey, Eqids, Ecids, Etrue = load_data(TEST_DIR)\n",
    "\n",
    "# Show some information about the shapes\n",
    "print(\"TX shape:\", TX.shape)\n",
    "print(\"Ty shape:\", Ty.shape)\n",
    "print(\"Tqids shape:\", Tqids.shape)\n",
    "print(\"Tcids shape:\", Tcids.shape)\n",
    "print(\"Ttrue shape:\", Ttrue.shape)\n",
    "print()\n",
    "print(\"VX shape:\", VX.shape)\n",
    "print(\"Vy shape:\", Vy.shape)\n",
    "print(\"Vqids shape:\", Vqids.shape)\n",
    "print(\"Vcids shape:\", Vcids.shape)\n",
    "print(\"Vtrue shape:\", Vtrue.shape)\n",
    "print()\n",
    "print(\"EX shape:\", EX.shape)\n",
    "print(\"Ey shape:\", Ey.shape)\n",
    "print(\"Eqids shape:\", Eqids.shape)\n",
    "print(\"Ecids shape:\", Ecids.shape)\n",
    "print(\"Etrue shape:\", Etrue.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics\n",
    "\n",
    "For evaluating the quality of results, we will use two metrics: [NDCG](https://en.wikipedia.org/wiki/Discounted_cumulative_gain#Normalized_DCG) which is based on ranked suggestions (if the top ranked suggestion is correct it will contribute more to the score than getting the 2nd or 3rd suggestion right) and [F1 score](https://en.wikipedia.org/wiki/F1_score) that only considers binary choices: either a concept is a subject of a document or it is not, there are no fuzzy options. \n",
    "\n",
    "For the **NDCG score**, we can simply use the implementation from the `pyltr` library. Here we limit the NDCG evaluation to the top 100 most highly ranked subjects, meaning that subjects ranked 101st, 101nd etc won't contribute to the score (their effect would be almost negligible anyway do to the discounting performed in NDCG calculations).\n",
    "\n",
    "For the **F1 score**, we can use the `scikit-learn` implementation. However, its usage is complicated by the fact that we are mostly dealing with ranked suggestions in the pyltr format, so we need to convert those into the multilabel prediction format understood by scikit-learn and further binarize the predictions using a thresholding strategy. For simplicity we will pick the top 5 suggested concepts, so we get an amount of subjects that is similar to the librarian-assigned ones (around 4.5 subjects per document), and compare those to the gold standard. The score for each document will be calculated individually and the final result will be the average of those scores (i.e. sample based average).\n",
    "\n",
    "Finally, we will define an evaluation function that is given ranked suggestions as well as gold standard subjects (in both pyltr and scikit-learn formats) and some auxiliary information (qids and cids). It will calculate both NDCG and F1 score, print them out, and also return the scores so that they can be stored for later plotting etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# initialize the NDCG metric implementation\n",
    "ndcg_metric = pyltr.metrics.NDCG(k=100)\n",
    "\n",
    "def ranking_to_multilabel(pred, qids, cids):\n",
    "    \"\"\"convert from pyltr ranking prediction format to sklearn multilabel prediction format\"\"\"\n",
    "    doc_labels = collections.OrderedDict()  # key: qid, val: NumPy 1D array with length n_concepts\n",
    "    for score, qid, cid in zip(pred, qids, cids):\n",
    "        if qid not in doc_labels:\n",
    "            doc_labels[qid] = np.zeros(n_concepts)\n",
    "        doc_labels[qid][cid] = score\n",
    "    return np.array(list(doc_labels.values()))\n",
    "\n",
    "def binarize_multilabel_pred(pred, k=5):\n",
    "    \"\"\"convert predictions with scores to a boolean matrix, taking the top K predictions per document\"\"\"\n",
    "    # I'm pretty sure this could be done easier, perhaps in a single NumPy operation, but I can't figure it out\n",
    "    # Using a loop instead, it's not going to take long anyway\n",
    "    binary_labels = np.zeros(pred.shape, dtype=np.bool)\n",
    "    pred_order = pred.argsort()[:,::-1]\n",
    "    for i, mask in enumerate(pred_order[:,:k]):\n",
    "        binary_labels[i,mask] = True\n",
    "    return binary_labels\n",
    "\n",
    "def evaluate(name, qids, y, y_true, pred, cids):\n",
    "    \"\"\"evaluate a ranking, calculating its NDCG and F1 score. Print and return the scores\"\"\"\n",
    "    # calculate NDCG score\n",
    "    ndcg = ndcg_metric.calc_mean(qids, y, pred)\n",
    "\n",
    "    # calculate F1 score\n",
    "    pred_labels = ranking_to_multilabel(pred, qids, cids)\n",
    "    f1 = f1_score(y_true, binarize_multilabel_pred(pred_labels), average='samples')\n",
    "    \n",
    "    print(\"{:>20}:\\tNDCG={:.4f}\\tF1={:.4f}\".format(name, ndcg, f1))\n",
    "    return ndcg, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline evaluation\n",
    "\n",
    "Now we are ready for some baseline evaluations: we can evaluate the individual algorithms as well as a simplistic combination method using the mean of assigned scores (the only one implemented in Annif v0.31.0). We will store the results for later too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               tfidf:\tNDCG=0.4907\tF1=0.2220\n",
      "            fasttext:\tNDCG=0.3865\tF1=0.1329\n",
      "                maui:\tNDCG=0.3945\tF1=0.1861\n",
      "                mean:\tNDCG=0.5432\tF1=0.2632\n"
     ]
    }
   ],
   "source": [
    "# initialize dictionary for collecting evaluation results\n",
    "eval_results = collections.OrderedDict()\n",
    "\n",
    "# evaluate the individual algorithms\n",
    "for algoid, name in enumerate(ALGORITHMS):\n",
    "    eval_results[name] = evaluate(name, Eqids, Ey, Etrue, EX[:,algoid], Ecids)\n",
    "\n",
    "# evaluate the combination of algorithms using mean of scores\n",
    "eval_results[\"mean\"] = evaluate(\"mean\", Eqids, Ey, Etrue, EX.mean(1), Ecids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that out of individual algorithms, TF-IDF performed best, followed by Maui and fastText. The mean of scores gave better results than any algorithm alone. The NDCG and F1 measures seem to agree on which methods are better than others, as they both ranked the methods in the same order.\n",
    "\n",
    "Note that the F1 scores may seem quite low (in some classification tasks F1 scores of 0.95 and more are not uncommon), but keep in mind that the task given to the algorithms is quite difficult: for each document, it should pick the 5 subjects (out of nearly 28000) that best describe that document. There are numerous ways of getting this wrong and only a few ways of picking a good set of subjects. So even a rather low F1 score of 0.2 means that around one out of five subjects was right - much better than chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: union-of-top-K\n",
    "## TODO: mean-of-normalized-scores (L1, L2, max)\n",
    "## TODO: PAV aka Isotonic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning to Rank\n",
    "\n",
    "Let's try a true machine learning based fusion approach. We will apply the state of the art Learning to Rank algorithm LambdaMART, which is typically used for ranking results in a search engine. In this case, we consider each document a \"query\" and the predicted subjects as \"results\" and will try to come up with the ideal ranking of those subjects so that gold standard subjects are ranked as high as possible while non-relevant subjects are ranked lower. The algorithm is given a ranking measure that it then attempts to optimize; we will use NDCG as the measure to optimize.\n",
    "\n",
    "A Learning to Rank algorithm requires features for the \"results\" (i.e. subjects in our case); here we will simply use the raw scores predicted by the subject indexing algorithms as the features.\n",
    "\n",
    "The LambdaMART algorithm is conveniently implemented in the `pyltr` library. This implementation has some advanced features: it can be connected to a monitor that periodically evaluates the learned model on validation data and keeps track of the learning progress. If the monitor notices that the learning has reached a plateau - no improvement in validation scores has been made in the last K training rounds - it can stop the learning as well as roll back the model to the state when it reached that plateau. This is called *early stopping* and *trimming*, respectively, and it should help guard against overfitting the model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter  Train score    Remaining                           Monitor Output \n",
      "    1 2485059027827.9087       67.92m      C:      0.6158 B:      0.6158 S:  0\n",
      "    2 2485059027827.9072       65.77m      C:      0.6111 B:      0.6158 S:  1\n",
      "    3 2357295130178.3208       65.16m      C:      0.6114 B:      0.6158 S:  2\n",
      "    4 2357295130178.3242       64.39m      C:      0.6130 B:      0.6158 S:  3\n",
      "    5 2357295130178.3257       63.20m      C:      0.6130 B:      0.6158 S:  4\n",
      "    6 2357295130178.3286       62.13m      C:      0.6139 B:      0.6158 S:  5\n",
      "    7 2357295130178.3286       61.30m      C:      0.6130 B:      0.6158 S:  6\n",
      "    8 2357295130178.3335       60.66m      C:      0.6159 B:      0.6159 S:  0\n",
      "    9 2485059027827.9248       59.84m      C:      0.6191 B:      0.6191 S:  0\n",
      "   10 2485059027827.9248       59.01m      C:      0.6187 B:      0.6191 S:  1\n",
      "   15 2485059027827.9507       55.28m      C:      0.6356 B:      0.6356 S:  0\n",
      "   20 2679180801228.5591       51.94m      C:      0.6516 B:      0.6516 S:  0\n",
      "   25 2679180801228.5669       48.97m      C:      0.6525 B:      0.6525 S:  0\n",
      "   30 3312378753338.7188       45.91m      C:      0.6563 B:      0.6577 S:  2\n",
      "   35 3537009301679.4819       42.76m      C:      0.6609 B:      0.6609 S:  0\n",
      "   40 3537009301679.4819       39.54m      C:      0.6666 B:      0.6666 S:  0\n",
      "   45 3537009301679.4834       36.28m      C:      0.6671 B:      0.6674 S:  2\n",
      "   50 3537009301679.4834       33.11m      C:      0.6669 B:      0.6678 S:  4\n",
      "   60 3537009301679.4834       26.55m      C:      0.6668 B:      0.6684 S:  3\n",
      "   70 3537009301679.4834       19.91m      C:      0.6677 B:      0.6686 S:  3\n",
      "   80 3537009301679.4849       13.28m      C:      0.6700 B:      0.6700 S:  0\n",
      "   90 3537009301679.4849        6.64m      C:      0.6706 B:      0.6714 S:  1\n",
      "Early termination at iteration  99\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyltr.models.lambdamart.LambdaMART at 0x7fa4b97cbcf8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a monitor for early stopping and trimming, using validation data\n",
    "monitor = pyltr.models.monitors.ValidationMonitor(\n",
    "    VX, Vy, Vqids, metric=ndcg_metric, stop_after=20)\n",
    "\n",
    "# create a LambdaMART model for learning a suitable ranking\n",
    "model = pyltr.models.LambdaMART(\n",
    "    metric=ndcg_metric,\n",
    "    n_estimators=100,\n",
    "    max_leaf_nodes=10,\n",
    "    min_samples_leaf=64,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# fit the model to training data\n",
    "model.fit(TX, Ty, Tqids, monitor=monitor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a model trained on the training documents and validated on the validation documents. First, since we spent so much time creating the model, let's save it for later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ltr-model.joblib']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the learned model for later use\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(model, 'ltr-model.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the model to predict subject rankings on the evaluation documents and evaluate how well it did compared to the other fusion approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 ltr:\tNDCG=0.5802\tF1=0.2795\n"
     ]
    }
   ],
   "source": [
    "Epred = model.predict(EX)\n",
    "eval_results[\"ltr\"] = evaluate(\"ltr\", Eqids, Ey, Etrue, Epred, Ecids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check how much the input from each algorithm contributed to the rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               tfidf importance: 0.2383\n",
      "            fasttext importance: 0.3477\n",
      "                maui importance: 0.4139\n"
     ]
    }
   ],
   "source": [
    "for feature, importance in zip(ALGORITHMS, model.feature_importances_):\n",
    "    print(\"{:>20} importance: {:.4f}\".format(feature, importance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: plot the results using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
